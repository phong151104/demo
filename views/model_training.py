"""
Trang Hu·∫•n Luy·ªán M√¥ H√¨nh - Model Training
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from utils.ui_components import show_processing_placeholder
from utils.session_state import init_session_state

def render():
    """Render trang hu·∫•n luy·ªán m√¥ h√¨nh"""
    init_session_state()
    
    st.markdown("## ü§ñ Hu·∫•n Luy·ªán M√¥ H√¨nh")
    st.markdown("Ch·ªçn v√† c·∫•u h√¨nh m√¥ h√¨nh Machine Learning ƒë·ªÉ d·ª± ƒëo√°n ƒëi·ªÉm t√≠n d·ª•ng.")
    
    # Check prerequisites
    if st.session_state.data is None:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng upload d·ªØ li·ªáu t·ª´ trang 'Upload & EDA'.")
        return
    
    if not st.session_state.selected_features:
        st.warning("‚ö†Ô∏è Ch∆∞a ch·ªçn ƒë·∫∑c tr∆∞ng. Vui l√≤ng ch·ªçn ƒë·∫∑c tr∆∞ng t·ª´ trang 'X·ª≠ L√Ω & Ch·ªçn Bi·∫øn'.")
        return
    
    # Check if train/test split exists
    if 'train_data' not in st.session_state or st.session_state.train_data is None:
        st.warning("‚ö†Ô∏è Ch∆∞a chia t·∫≠p d·ªØ li·ªáu. Vui l√≤ng chia t·∫≠p Train/Valid/Test ·ªü trang 'X·ª≠ L√Ω & Ch·ªçn Bi·∫øn'.")
        return
    
    st.success(f"‚úÖ S·∫µn s√†ng hu·∫•n luy·ªán v·ªõi {len(st.session_state.selected_features)} ƒë·∫∑c tr∆∞ng")
    
    st.markdown("---")
    
    # Initialize model history
    if 'model_history' not in st.session_state:
        st.session_state.model_history = []

    # Tabs
    tab1, tab2, tab3 = st.tabs([
        "‚öôÔ∏è C·∫•u H√¨nh M√¥ H√¨nh",
        "üìä K·∫øt Qu·∫£ ƒê√°nh Gi√°",
        "üìà So S√°nh M√¥ H√¨nh"
    ])
    
    # Tab 1: Model Configuration
    with tab1:
        st.markdown("### ‚öôÔ∏è C·∫•u H√¨nh V√† Hu·∫•n Luy·ªán")
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.markdown("#### 1Ô∏è‚É£ Ch·ªçn M√¥ H√¨nh")
            
            model_type = st.selectbox(
                "Lo·∫°i m√¥ h√¨nh:",
                [
                    "Logistic Regression",
                    "Random Forest",
                    "XGBoost",
                    "LightGBM",
                    "CatBoost",
                    "Gradient Boosting"
                ],
                key="model_type"
            )
            
            st.markdown("---")
            
            st.markdown("#### 2Ô∏è‚É£ D·ªØ Li·ªáu Hu·∫•n Luy·ªán")
            
            st.info(f"üìä T·∫≠p Train: {len(st.session_state.train_data)} d√≤ng")
            if 'test_data' in st.session_state and st.session_state.test_data is not None:
                st.info(f"üß™ T·∫≠p Test: {len(st.session_state.test_data)} d√≤ng")
            
            st.markdown("---")
            
            st.markdown("#### 3Ô∏è‚É£ Tham S·ªë M√¥ H√¨nh")
            
            # Model-specific parameters
            if model_type == "Logistic Regression":
                c_value = st.slider("C (Regularization):", 0.001, 10.0, 1.0, 0.001, key="lr_c")
                max_iter = st.number_input("Max iterations:", 100, 1000, 200, key="lr_iter")
                
            elif model_type == "Random Forest":
                n_estimators = st.slider("S·ªë c√¢y (n_estimators):", 50, 500, 100, 10, key="n_trees")
                max_depth = st.slider("ƒê·ªô s√¢u t·ªëi ƒëa:", 3, 20, 10, 1, key="max_depth")
                
            elif model_type in ["XGBoost", "LightGBM", "CatBoost", "Gradient Boosting"]:
                n_estimators = st.slider("S·ªë c√¢y (n_estimators):", 50, 500, 100, 10, key="n_trees")
                max_depth = st.slider("ƒê·ªô s√¢u t·ªëi ƒëa:", 3, 20, 6, 1, key="max_depth")
                learning_rate = st.slider("Learning rate:", 0.01, 0.3, 0.1, 0.01, key="lr")
                subsample = st.slider("Subsample:", 0.5, 1.0, 0.8, 0.1, key="subsample")
            
            st.markdown("---")
            
            # Train button
            if st.button("üöÄ Hu·∫•n Luy·ªán M√¥ H√¨nh", type="primary", use_container_width=True):
                try:
                    with st.spinner(f"ƒêang hu·∫•n luy·ªán {model_type}..."):
                        show_processing_placeholder(f"Train {model_type} v·ªõi {len(st.session_state.selected_features)} features")
                        
                        # Prepare data
                        target_col = st.session_state.target_column
                        features = st.session_state.selected_features
                        
                        X_train = st.session_state.train_data[features]
                        y_train = st.session_state.train_data[target_col]
                        
                        # Use test data if available, otherwise split train data (fallback)
                        if 'test_data' in st.session_state and st.session_state.test_data is not None:
                            X_test = st.session_state.test_data[features]
                            y_test = st.session_state.test_data[target_col]
                        else:
                            # Fallback if no test data (should not happen if flow is followed)
                            from sklearn.model_selection import train_test_split
                            X_train, X_test, y_train, y_test = train_test_split(
                                X_train, y_train, test_size=0.2, random_state=42
                            )
                        
                        # Collect parameters
                        params = {}
                        if model_type == "Logistic Regression":
                            params['C'] = c_value
                            params['max_iter'] = max_iter
                        elif model_type == "Random Forest":
                            params['n_estimators'] = n_estimators
                            params['max_depth'] = max_depth
                        elif model_type in ["XGBoost", "LightGBM", "CatBoost", "Gradient Boosting"]:
                            params['n_estimators'] = n_estimators
                            params['max_depth'] = max_depth
                            params['learning_rate'] = learning_rate
                            params['subsample'] = subsample
                        
                        # Import backend
                        from backend.models.trainer import train_model
                        
                        # Train model
                        model, metrics = train_model(
                            X_train, y_train, X_test, y_test, 
                            model_type, params
                        )
                        
                        # Save model info to session
                        # st.session_state.model_type is already updated by the widget
                        st.session_state.model = model
                        st.session_state.model_metrics = metrics
                        
                        # Add to history
                        import datetime
                        history_entry = {
                            'Model': model_type,
                            'Accuracy': metrics['accuracy'],
                            'Precision': metrics['precision'],
                            'Recall': metrics['recall'],
                            'F1-Score': metrics['f1'],
                            'AUC': metrics['auc'],
                            'Timestamp': datetime.datetime.now().strftime("%H:%M:%S"),
                            'Params': str(params)
                        }
                        st.session_state.model_history.append(history_entry)
                        
                        st.success(f"‚úÖ ƒê√£ hu·∫•n luy·ªán {model_type} th√†nh c√¥ng!")
                        st.balloons()
                        st.rerun()
                        
                except Exception as e:
                    st.error(f"‚ùå L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh: {str(e)}")
                    import traceback
                    with st.expander("Chi ti·∫øt l·ªói"):
                        st.code(traceback.format_exc())
        
        with col2:
            st.markdown("#### üìã Th√¥ng Tin Hu·∫•n Luy·ªán")
            
            # Training info panel
            st.markdown("""
            <div style="background-color: #262730; padding: 1.5rem; border-radius: 10px; margin-bottom: 1rem;">
                <h4 style="margin-top: 0; color: #667eea;">üí° H∆∞·ªõng D·∫´n</h4>
                <ul style="margin-bottom: 0;">
                    <li><strong>Logistic Regression</strong>: Nhanh, d·ªÖ gi·∫£i th√≠ch, ph√π h·ª£p v·ªõi d·ªØ li·ªáu tuy·∫øn t√≠nh</li>
                    <li><strong>Random Forest</strong>: M·∫°nh m·∫Ω, ch·ªëng overfitting t·ªët</li>
                    <li><strong>XGBoost/LightGBM</strong>: Hi·ªáu su·∫•t cao, th∆∞·ªùng cho k·∫øt qu·∫£ t·ªët nh·∫•t</li>
                    <li><strong>CatBoost</strong>: T·ªët v·ªõi bi·∫øn ph√¢n lo·∫°i, √≠t c·∫ßn ti·ªÅn x·ª≠ l√Ω</li>
                </ul>
            </div>
            """, unsafe_allow_html=True)
            
            # Current configuration summary
            if st.session_state.model is not None:
                st.markdown("#### ‚úÖ M√¥ H√¨nh Hi·ªán T·∫°i")
                
                st.markdown(f"""
                <div style="background: linear-gradient(135deg, #1f2937 0%, #345f9c 100%); 
                            padding: 1.5rem; border-radius: 10px;">
                    <h3 style="margin: 0; color: white;">{st.session_state.model_type}</h3>
                    <p style="margin: 0.5rem 0 0 0; color: rgba(255,255,255,0.9);">
                        ƒê√£ hu·∫•n luy·ªán v·ªõi {len(st.session_state.selected_features)} features
                    </p>
                </div>
                """, unsafe_allow_html=True)
                
                st.markdown("<br>", unsafe_allow_html=True)
                
                # Quick metrics
                metrics = st.session_state.model_metrics
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Accuracy", f"{metrics['accuracy']:.3f}")
                with col2:
                    st.metric("Precision", f"{metrics['precision']:.3f}")
                with col3:
                    st.metric("AUC", f"{metrics['auc']:.3f}")
            else:
                st.info("‚è≥ Ch∆∞a hu·∫•n luy·ªán m√¥ h√¨nh n√†o")
            
            # Additional options
            st.markdown("#### ‚öôÔ∏è T√πy Ch·ªçn N√¢ng Cao")
            
            with st.expander("Cross-Validation"):
                cv_folds = st.slider("S·ªë folds:", 3, 10, 5, key="cv_folds")
                if st.button("üîÑ Ch·∫°y Cross-Validation", key="run_cv"):
                    show_processing_placeholder(f"Cross-validation v·ªõi {cv_folds} folds")
                    st.success(f"‚úÖ CV Score: {np.random.uniform(0.75, 0.90):.3f} (+/- {np.random.uniform(0.02, 0.05):.3f})")
            
            with st.expander("Hyperparameter Tuning"):
                tuning_method = st.selectbox(
                    "Ph∆∞∆°ng ph√°p:",
                    ["Grid Search", "Random Search", "Bayesian Optimization"],
                    key="tuning_method"
                )
                if st.button("üîç T√¨m Tham S·ªë T·ªët Nh·∫•t", key="tune_params"):
                    show_processing_placeholder(f"Hyperparameter tuning v·ªõi {tuning_method}")
                    st.success("‚úÖ ƒê√£ t√¨m ƒë∆∞·ª£c tham s·ªë t·ªët nh·∫•t!")
    
    # Tab 2: Evaluation Results
    with tab2:
        st.markdown("### üìä K·∫øt Qu·∫£ ƒê√°nh Gi√° M√¥ H√¨nh")
        
        if st.session_state.model is None:
            st.warning("‚ö†Ô∏è Ch∆∞a c√≥ m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán. Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc.")
            return
        
        metrics = st.session_state.model_metrics
        
        # Metrics overview
        st.markdown("#### üìà C√°c Ch·ªâ S·ªë ƒê√°nh Gi√°")
        
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.metric("Accuracy", f"{metrics['accuracy']:.3f}")
        with col2:
            st.metric("Precision", f"{metrics['precision']:.3f}")
        with col3:
            st.metric("Recall", f"{metrics['recall']:.3f}")
        with col4:
            st.metric("F1-Score", f"{metrics['f1']:.3f}")
        with col5:
            st.metric("AUC", f"{metrics['auc']:.3f}")
        
        st.markdown("---")
        
        # Visualizations
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìä Confusion Matrix")
            
            # Mock confusion matrix
            cm = np.array([
                [850, 150],
                [100, 900]
            ])
            
            fig = px.imshow(
                cm,
                labels=dict(x="Predicted", y="Actual", color="Count"),
                x=['Negative (0)', 'Positive (1)'],
                y=['Negative (0)', 'Positive (1)'],
                text_auto=True,
                color_continuous_scale='Blues',
                title="Confusion Matrix"
            )
            
            fig.update_layout(
                template="plotly_dark",
                height=400
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Metrics explanation
            st.markdown("""
            <div style="background-color: #262730; padding: 1rem; border-radius: 8px;">
                <p style="margin: 0; font-size: 0.9rem;">
                    <strong>True Negative:</strong> {0} | <strong>False Positive:</strong> {1}<br>
                    <strong>False Negative:</strong> {2} | <strong>True Positive:</strong> {3}
                </p>
            </div>
            """.format(cm[0,0], cm[0,1], cm[1,0], cm[1,1]), unsafe_allow_html=True)
        
        with col2:
            st.markdown("#### üìà ROC Curve")
            
            # Mock ROC curve
            fpr = np.linspace(0, 1, 100)
            tpr = np.sqrt(fpr) * metrics['auc'] + np.random.normal(0, 0.02, 100)
            tpr = np.clip(tpr, 0, 1)
            
            fig = go.Figure()
            
            # ROC curve
            fig.add_trace(go.Scatter(
                x=fpr,
                y=tpr,
                mode='lines',
                name=f'{st.session_state.model_type} (AUC = {metrics["auc"]:.3f})',
                line=dict(color='#667eea', width=3)
            ))
            
            # Diagonal line
            fig.add_trace(go.Scatter(
                x=[0, 1],
                y=[0, 1],
                mode='lines',
                name='Random (AUC = 0.5)',
                line=dict(color='red', width=2, dash='dash')
            ))
            
            fig.update_layout(
                title='ROC Curve',
                xaxis_title='False Positive Rate',
                yaxis_title='True Positive Rate',
                template="plotly_dark",
                height=400,
                showlegend=True,
                legend=dict(x=0.6, y=0.1)
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        st.markdown("---")
        
        # Detailed metrics table
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìã Classification Report")
            
            report_data = pd.DataFrame({
                'Class': ['Negative (0)', 'Positive (1)', 'Weighted Avg'],
                'Precision': [
                    metrics['precision'] - 0.02,
                    metrics['precision'] + 0.02,
                    metrics['precision']
                ],
                'Recall': [
                    metrics['recall'] + 0.01,
                    metrics['recall'] - 0.01,
                    metrics['recall']
                ],
                'F1-Score': [
                    metrics['f1'] - 0.01,
                    metrics['f1'] + 0.01,
                    metrics['f1']
                ],
                'Support': [1000, 1000, 2000]
            })
            
            st.dataframe(
                report_data.style.format({
                    'Precision': '{:.3f}',
                    'Recall': '{:.3f}',
                    'F1-Score': '{:.3f}'
                }).background_gradient(subset=['Precision', 'Recall', 'F1-Score'], cmap='Greens'),
                use_container_width=True
            )
        
        with col2:
            st.markdown("#### üìä Precision-Recall Curve")
            
            # Mock PR curve
            recall_vals = np.linspace(0, 1, 100)
            precision_vals = 1 - recall_vals * 0.3 + np.random.normal(0, 0.02, 100)
            precision_vals = np.clip(precision_vals, 0, 1)
            
            fig = go.Figure()
            
            fig.add_trace(go.Scatter(
                x=recall_vals,
                y=precision_vals,
                mode='lines',
                fill='tozeroy',
                name=st.session_state.model_type,
                line=dict(color='#764ba2', width=3)
            ))
            
            fig.update_layout(
                title='Precision-Recall Curve',
                xaxis_title='Recall',
                yaxis_title='Precision',
                template="plotly_dark",
                height=300
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        # Download results
        st.markdown("---")
        col1, col2, col3 = st.columns([1, 1, 1])
        
        with col2:
            if st.button("üíæ L∆∞u M√¥ H√¨nh", use_container_width=True):
                show_processing_placeholder("L∆∞u m√¥ h√¨nh v√†o file .pkl")
                st.success("‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh!")
    
    # Tab 3: Model Comparison
    with tab3:
        st.markdown("### üìà L·ªãch S·ª≠ & So S√°nh M√¥ H√¨nh")
        
        if not st.session_state.model_history:
            st.info("üí° Ch∆∞a c√≥ l·ªãch s·ª≠ hu·∫•n luy·ªán. H√£y hu·∫•n luy·ªán √≠t nh·∫•t m·ªôt m√¥ h√¨nh ·ªü Tab 1.")
        else:
            st.markdown(f"ƒê√£ l∆∞u {len(st.session_state.model_history)} k·∫øt qu·∫£ hu·∫•n luy·ªán.")
            
            # Convert history to DataFrame
            history_df = pd.DataFrame(st.session_state.model_history)
            
            # Display table
            st.markdown("#### üìä B·∫£ng So S√°nh Chi Ti·∫øt")
            st.dataframe(
                history_df.style.format({
                    'Accuracy': '{:.3f}',
                    'Precision': '{:.3f}',
                    'Recall': '{:.3f}',
                    'F1-Score': '{:.3f}',
                    'AUC': '{:.3f}'
                }).background_gradient(subset=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'], cmap='RdYlGn'),
                use_container_width=True
            )
            
            # Comparison charts
            st.markdown("#### üìâ Bi·ªÉu ƒê·ªì So S√°nh")
            col1, col2 = st.columns(2)
            
            with col1:
                # Bar chart comparison
                fig = go.Figure()
                
                metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
                
                # Group by Model to get average or max if multiple runs of same model? 
                # For now, just plot all runs, maybe add ID or Timestamp to x-axis
                history_df['Run ID'] = history_df['Model'] + " (" + history_df['Timestamp'] + ")"
                
                for metric in metrics_to_plot:
                    fig.add_trace(go.Bar(
                        name=metric,
                        x=history_df['Run ID'],
                        y=history_df[metric],
                        text=history_df[metric].round(3),
                        textposition='outside'
                    ))
                
                fig.update_layout(
                    title='So S√°nh Metrics Gi·ªØa C√°c L·∫ßn Ch·∫°y',
                    xaxis_title='Model Run',
                    yaxis_title='Score',
                    template="plotly_dark",
                    height=400,
                    barmode='group'
                )
                
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Line chart for AUC trend
                fig = px.line(
                    history_df, 
                    x='Timestamp', 
                    y='AUC', 
                    color='Model',
                    markers=True,
                    title='Xu H∆∞·ªõng AUC Theo Th·ªùi Gian'
                )
                fig.update_layout(template="plotly_dark", height=400)
                st.plotly_chart(fig, use_container_width=True)
            
            # Best model recommendation
            best_run_idx = history_df['AUC'].idxmax()
            best_model = history_df.loc[best_run_idx, 'Model']
            best_auc = history_df.loc[best_run_idx, 'AUC']
            best_time = history_df.loc[best_run_idx, 'Timestamp']
            
            st.success(f"üèÜ **M√¥ h√¨nh t·ªët nh·∫•t hi·ªán t·∫°i**: {best_model} (ch·∫°y l√∫c {best_time}) v·ªõi AUC = {best_auc:.3f}")
            
            # Clear history button
            if st.button("üóëÔ∏è X√≥a L·ªãch S·ª≠", type="secondary"):
                st.session_state.model_history = []
                st.rerun()

