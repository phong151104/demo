"""
Trang Hu·∫•n Luy·ªán M√¥ H√¨nh - Model Training
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from utils.session_state import init_session_state
from utils.permissions import check_and_show_view_only, has_permission

def render():
    """Render trang hu·∫•n luy·ªán m√¥ h√¨nh"""
    init_session_state()
    
    st.markdown("## üß† Hu·∫•n Luy·ªán M√¥ H√¨nh")
    st.markdown("Ch·ªçn v√† c·∫•u h√¨nh m√¥ h√¨nh Machine Learning ƒë·ªÉ d·ª± ƒëo√°n ƒëi·ªÉm t√≠n d·ª•ng.")
    
    # Check view-only mode
    is_view_only = check_and_show_view_only("üß† Model Training")
    
    # Check prerequisites
    if st.session_state.data is None:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng upload d·ªØ li·ªáu t·ª´ trang 'Upload & EDA'.")
        return
    
    if not st.session_state.selected_features:
        st.warning("‚ö†Ô∏è Ch∆∞a ch·ªçn ƒë·∫∑c tr∆∞ng. Vui l√≤ng ch·ªçn ƒë·∫∑c tr∆∞ng t·ª´ trang 'X·ª≠ L√Ω & Ch·ªçn Bi·∫øn'.")
        return
    
    # Check if train/test split exists
    if 'train_data' not in st.session_state or st.session_state.train_data is None:
        st.warning("‚ö†Ô∏è Ch∆∞a chia t·∫≠p d·ªØ li·ªáu. Vui l√≤ng chia t·∫≠p Train/Valid/Test ·ªü trang 'X·ª≠ L√Ω & Ch·ªçn Bi·∫øn'.")
        return
    
    st.success(f"‚úÖ S·∫µn s√†ng hu·∫•n luy·ªán v·ªõi {len(st.session_state.selected_features)} ƒë·∫∑c tr∆∞ng")
    
    st.markdown("---")
    
    # Initialize model history
    if 'model_history' not in st.session_state:
        st.session_state.model_history = []

    # Tabs
    tab1, tab2, tab3 = st.tabs([
        "‚öôÔ∏è C·∫•u H√¨nh M√¥ H√¨nh",
        "üìä K·∫øt Qu·∫£ ƒê√°nh Gi√°",
        "üìà So S√°nh M√¥ H√¨nh"
    ])
    
    # Tab 1: Model Configuration
    with tab1:
        st.markdown("### ‚öôÔ∏è C·∫•u H√¨nh V√† Hu·∫•n Luy·ªán")
        
        # Initialize training flag
        if '_training_in_progress' not in st.session_state:
            st.session_state._training_in_progress = False
        
        # N·∫øu ƒëang training - hi·ªÉn th·ªã spinner TR∆Ø·ªöC, kh√¥ng render form
        if st.session_state._training_in_progress:
            model_type = st.session_state._training_model_type
            params = st.session_state._training_params
            
            # Hi·ªÉn th·ªã animation loading ·ªü gi·ªØa m√†n h√¨nh v·ªõi CSS spinner
            st.markdown("""
            <style>
            @keyframes spin {
                0% { transform: rotate(0deg); }
                100% { transform: rotate(360deg); }
            }
            .training-spinner {
                width: 60px;
                height: 60px;
                border: 5px solid rgba(102, 126, 234, 0.2);
                border-top: 5px solid #667eea;
                border-radius: 50%;
                animation: spin 1s linear infinite;
                margin: 0 auto 1.5rem auto;
            }
            </style>
            <div style="display: flex; flex-direction: column; align-items: center; justify-content: center; padding: 3rem;">
                <div class="training-spinner"></div>
                <h3 style="color: #667eea; margin-bottom: 0.5rem;">ƒêang Hu·∫•n Luy·ªán M√¥ H√¨nh</h3>
                <p style="color: rgba(255,255,255,0.7);">""" + model_type + """</p>
            </div>
            """, unsafe_allow_html=True)
            
            progress_placeholder = st.empty()
            progress_placeholder.info("‚è≥ ƒêang x·ª≠ l√Ω... vui l√≤ng ƒë·ª£i")
            
            try:
                # Prepare data
                target_col = st.session_state.target_column
                features = st.session_state.selected_features
                
                X_train = st.session_state.train_data[features]
                y_train = st.session_state.train_data[target_col]
                
                # Use test data if available
                if 'test_data' in st.session_state and st.session_state.test_data is not None:
                    X_test = st.session_state.test_data[features]
                    y_test = st.session_state.test_data[target_col]
                else:
                    from sklearn.model_selection import train_test_split
                    X_train, X_test, y_train, y_test = train_test_split(
                        X_train, y_train, test_size=0.2, random_state=42
                    )
                
                # Extract validation data if available (for early stopping and metrics comparison)
                X_valid, y_valid = None, None
                if 'valid_data' in st.session_state and st.session_state.valid_data is not None:
                    X_valid = st.session_state.valid_data[features]
                    y_valid = st.session_state.valid_data[target_col]
                    # Store in session state for UI display
                    st.session_state.X_valid = X_valid
                    st.session_state.y_valid = y_valid
                
                # Import backend
                from backend.models.trainer import train_model, train_stacking_model, tune_stacking_with_oof
                
                # Train model - special handling for Stacking
                if model_type == "Stacking Ensemble":
                    base_models = params.get('base_models', ['LR', 'DT'])
                    meta_model = params.get('meta_model', 'Random Forest')
                    enable_tuning = params.get('enable_tuning', False)
                    
                    if enable_tuning:
                        # Use OOF tuning approach
                        tuning_method = params.get('tuning_method', 'Grid Search')
                        cv_folds = params.get('cv_folds', 5)
                        base_model_params = params.get('base_model_params', {})
                        
                        model, metrics, tuning_info = tune_stacking_with_oof(
                            X_train, y_train, X_test, y_test,
                            base_models_config=base_model_params,
                            meta_model=meta_model,
                            tuning_method=tuning_method,
                            n_folds=cv_folds,
                            params=params
                        )
                        # Store tuning info
                        st.session_state.stacking_tuning_info = tuning_info
                    else:
                        # Standard stacking without tuning
                        model, metrics = train_stacking_model(
                            X_train, y_train, X_test, y_test,
                            base_models=base_models,
                            meta_model=meta_model,
                            params=params
                        )
                else:
                    # Get validation data if exists (for early stopping and metrics comparison)
                    X_valid = st.session_state.get('X_valid')
                    y_valid = st.session_state.get('y_valid')
                    early_stopping_rounds = params.get('early_stopping_rounds')
                    
                    model, metrics = train_model(
                        X_train, y_train, X_test, y_test, 
                        model_type, params,
                        X_valid=X_valid,
                        y_valid=y_valid,
                        early_stopping_rounds=early_stopping_rounds
                    )
                
                # Save model info to session
                st.session_state.model = model
                st.session_state.model_metrics = metrics
                st.session_state.selected_model_name = model_type
                
                # Add to history
                import datetime
                history_entry = {
                    'Model': model_type,
                    'Accuracy': metrics['accuracy'],
                    'Precision': metrics['precision'],
                    'Recall': metrics['recall'],
                    'F1-Score': metrics['f1'],
                    'AUC': metrics['auc'],
                    'Timestamp': datetime.datetime.now().strftime("%H:%M:%S"),
                    'Params': str(params)
                }
                st.session_state.model_history.append(history_entry)
                
                # Auto-select the newly trained model
                st.session_state.selected_model_idx = len(st.session_state.model_history) - 1
                st.session_state.selected_model_timestamp = history_entry['Timestamp']
                
                st.session_state._training_success = True
                st.session_state._training_model_name = model_type
                
            except Exception as e:
                st.session_state._training_error = str(e)
                import traceback
                st.session_state._training_traceback = traceback.format_exc()
            
            # Reset flag v√† rerun
            st.session_state._training_in_progress = False
            st.rerun()
        
        else:
            # Khi kh√¥ng ƒëang training - hi·ªÉn th·ªã form b√¨nh th∆∞·ªùng
            col1, col2 = st.columns([1, 2])
            
            with col1:
                # Hi·ªÉn th·ªã th√¥ng b√°o k·∫øt qu·∫£ t·ª´ l·∫ßn training tr∆∞·ªõc
                if st.session_state.get('_training_success', False):
                    st.success(f"‚úÖ ƒê√£ hu·∫•n luy·ªán {st.session_state._training_model_name} th√†nh c√¥ng!")
                    st.session_state._training_success = False
                    del st.session_state._training_model_name
                
                if st.session_state.get('_training_error', None):
                    st.error(f"‚ùå L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh: {st.session_state._training_error}")
                    with st.expander("Chi ti·∫øt l·ªói"):
                        st.code(st.session_state._training_traceback)
                    st.session_state._training_error = None
                    st.session_state._training_traceback = None
                
                # Show success message for applied best params
                if st.session_state.get('_params_applied_success', False):
                    st.success("‚úÖ ƒê√£ √°p d·ª•ng tham s·ªë t·ªët nh·∫•t! B·∫°n c√≥ th·ªÉ hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh.")
                    st.session_state._params_applied_success = False
                
                st.markdown("#### 1Ô∏è‚É£ Ch·ªçn M√¥ H√¨nh")
                
                # Define model list
                model_list = [
                    "Logistic Regression",
                    "Random Forest",
                    "XGBoost",
                    "LightGBM",
                    "CatBoost",
                    "Gradient Boosting",
                    "Stacking Ensemble"
                ]
                
                # Get default index from last trained model or previous selection
                default_idx = 0
                last_trained_model = st.session_state.get('_training_model_type', None)
                prev_selected = st.session_state.get('model_type_select', None)
                
                if prev_selected and prev_selected in model_list:
                    default_idx = model_list.index(prev_selected)
                elif last_trained_model and last_trained_model in model_list:
                    default_idx = model_list.index(last_trained_model)
                
                model_type = st.selectbox(
                    "Lo·∫°i m√¥ h√¨nh:",
                    model_list,
                    index=default_idx,
                    key="model_type_select"
                )
                
                st.markdown("---")
                
                st.markdown("#### 2Ô∏è‚É£ D·ªØ Li·ªáu Hu·∫•n Luy·ªán")
                
                st.info(f"üìä T·∫≠p Train: {len(st.session_state.train_data)} d√≤ng")
                
                # Show Validation set if exists (check valid_data from feature_engineering)
                if st.session_state.get('valid_data') is not None:
                    valid_len = len(st.session_state.valid_data)
                    st.info(f"‚úÖ T·∫≠p Validation: {valid_len} d√≤ng")
                
                if 'test_data' in st.session_state and st.session_state.test_data is not None:
                    st.info(f"üß™ T·∫≠p Test: {len(st.session_state.test_data)} d√≤ng")
                
                st.markdown("---")
                
                st.markdown("#### 3Ô∏è‚É£ Tham S·ªë M√¥ H√¨nh")
                
                # Detect model type change and clear widget keys to avoid conflicts
                # ONLY clear if user actively changed model (not on first load or after training)
                prev_model_type = st.session_state.get('_prev_model_type', None)
                if prev_model_type is not None and prev_model_type != model_type:
                    # User actually changed model - clear parameter-related keys
                    keys_to_clear = ['n_trees', 'max_depth', 'lr', 'subsample', 'min_samples_split', 
                                    'unlimited_depth', 'lr_c', 'lr_iter']
                    for key in keys_to_clear:
                        if key in st.session_state:
                            del st.session_state[key]
                    # Also clear best_tuned_params if model changed
                    if 'best_tuned_params' in st.session_state:
                        del st.session_state['best_tuned_params']
                    if 'tuning_results' in st.session_state:
                        del st.session_state['tuning_results']
                
                # Always update prev model type to current selection
                st.session_state._prev_model_type = model_type
                
                # Show applied params notification
                if st.session_state.get('best_tuned_params') and st.session_state.get('_params_applied_success', False):
                    applied_params = st.session_state.best_tuned_params
                    params_str = ", ".join([f"**{k}**: `{v}`" for k, v in applied_params.items()])
                    st.success(f"‚úÖ ƒê√£ √°p d·ª•ng tham s·ªë t·ªët nh·∫•t: {params_str}")
                    st.session_state._params_applied_success = False
                
                # Get tuned params if available
                tuned_params = st.session_state.get('best_tuned_params', {})
                
                # Check if we need to apply tuned params - SET widget keys DIRECTLY
                if st.session_state.get('apply_tuned_params_flag', False):
                    # For Logistic Regression
                    if 'C' in tuned_params:
                        c_val = max(0.001, min(10.0, tuned_params['C']))
                        st.session_state.lr_c = c_val
                    if 'max_iter' in tuned_params:
                        iter_val = max(100, min(1000, tuned_params['max_iter']))
                        st.session_state.lr_iter = iter_val
                    # For tree-based models
                    if 'n_estimators' in tuned_params:
                        st.session_state.n_trees = max(50, min(500, tuned_params['n_estimators']))
                    if 'max_depth' in tuned_params:
                        depth = tuned_params['max_depth']
                        if depth == -1 or depth is None:
                            st.session_state.unlimited_depth = True
                        else:
                            st.session_state.max_depth = max(3, min(20, depth))
                            st.session_state.unlimited_depth = False
                    if 'learning_rate' in tuned_params:
                        st.session_state.lr = max(0.01, min(0.3, tuned_params['learning_rate']))
                    if 'subsample' in tuned_params:
                        st.session_state.subsample = max(0.5, min(1.0, tuned_params['subsample']))
                    if 'min_samples_split' in tuned_params:
                        st.session_state.min_samples_split = max(2, min(20, tuned_params['min_samples_split']))
                    
                    st.session_state.apply_tuned_params_flag = False
                    st.session_state._params_applied_success = True
                
                # Model-specific parameters - collect params
                params = {}
                
                if model_type == "Logistic Regression":
                    # Only set default if key not in session_state (avoids Streamlit warning)
                    if 'lr_c' not in st.session_state:
                        st.session_state.lr_c = max(0.001, min(10.0, tuned_params.get('C', 1.0)))
                    if 'lr_iter' not in st.session_state:
                        st.session_state.lr_iter = max(100, min(1000, tuned_params.get('max_iter', 200)))
                    
                    c_value = st.slider("C (Regularization):", 0.001, 10.0, step=0.001, key="lr_c")
                    max_iter = st.number_input("Max iterations:", 100, 1000, key="lr_iter")
                    params['C'] = c_value
                    params['max_iter'] = max_iter
                    
                elif model_type == "Random Forest":
                    default_n_estimators = tuned_params.get('n_estimators', 100)
                    default_max_depth = tuned_params.get('max_depth', 10)
                    default_min_samples_split = tuned_params.get('min_samples_split', 2)
                    # Clamp values
                    default_n_estimators = max(50, min(500, default_n_estimators))
                    if default_max_depth is None:
                        default_max_depth = 10
                    default_max_depth = max(3, min(20, default_max_depth))
                    default_min_samples_split = max(2, min(20, default_min_samples_split))
                    
                    # Set session state only if key doesn't exist (avoids Streamlit widget conflict)
                    if 'n_trees' not in st.session_state:
                        st.session_state.n_trees = int(default_n_estimators)
                    if 'max_depth' not in st.session_state:
                        st.session_state.max_depth = int(default_max_depth)
                    if 'min_samples_split' not in st.session_state:
                        st.session_state.min_samples_split = int(default_min_samples_split)
                    
                    n_estimators = st.slider("S·ªë c√¢y (n_estimators):", 50, 500, step=10, key="n_trees")
                    max_depth = st.slider("ƒê·ªô s√¢u t·ªëi ƒëa:", 3, 20, step=1, key="max_depth")
                    min_samples_split = st.slider("Min samples split:", 2, 20, step=1, key="min_samples_split")
                    params['n_estimators'] = n_estimators
                    params['max_depth'] = max_depth
                    params['min_samples_split'] = min_samples_split
                    
                elif model_type in ["XGBoost", "LightGBM", "CatBoost", "Gradient Boosting"]:
                    default_n_estimators = tuned_params.get('n_estimators', tuned_params.get('iterations', 100))
                    default_max_depth = tuned_params.get('max_depth', tuned_params.get('depth', 6))
                    default_learning_rate = tuned_params.get('learning_rate', 0.1)
                    default_subsample = tuned_params.get('subsample', 0.8)
                    
                    # Check if max_depth is unlimited (-1 or None)
                    is_unlimited_depth = default_max_depth == -1 or default_max_depth is None
                    
                    # Clamp values
                    default_n_estimators = max(50, min(500, default_n_estimators))
                    if default_max_depth is None or default_max_depth == -1:
                        default_max_depth = 6  # Default for slider display
                    default_max_depth = max(3, min(20, default_max_depth))
                    default_learning_rate = max(0.01, min(0.3, default_learning_rate))
                    default_subsample = max(0.5, min(1.0, default_subsample))
                    
                    # Set session state only if key doesn't exist (avoids Streamlit widget conflict)
                    if 'n_trees' not in st.session_state:
                        st.session_state.n_trees = int(default_n_estimators)
                    if 'max_depth' not in st.session_state:
                        st.session_state.max_depth = int(default_max_depth)
                    if 'lr' not in st.session_state:
                        st.session_state.lr = float(default_learning_rate)
                    if 'subsample' not in st.session_state:
                        st.session_state.subsample = float(default_subsample)
                    if 'unlimited_depth' not in st.session_state:
                        st.session_state.unlimited_depth = is_unlimited_depth
                    
                    n_estimators = st.slider("S·ªë c√¢y (n_estimators):", 50, 500, step=10, key="n_trees")
                    
                    # For LightGBM, allow unlimited depth option
                    if model_type == "LightGBM":
                        unlimited_depth = st.checkbox(
                            "üîì Kh√¥ng gi·ªõi h·∫°n ƒë·ªô s√¢u (max_depth = -1)", 
                            key="unlimited_depth",
                            help="Trong LightGBM, max_depth=-1 cho ph√©p c√¢y ph√°t tri·ªÉn kh√¥ng gi·ªõi h·∫°n. C√≥ th·ªÉ tƒÉng hi·ªáu su·∫•t nh∆∞ng c≈©ng tƒÉng nguy c∆° overfitting."
                        )
                        if unlimited_depth:
                            max_depth = -1
                            st.info("üí° ƒê·ªô s√¢u kh√¥ng gi·ªõi h·∫°n - c√¢y s·∫Ω ph√°t tri·ªÉn d·ª±a tr√™n num_leaves")
                        else:
                            max_depth = st.slider("ƒê·ªô s√¢u t·ªëi ƒëa:", 3, 20, step=1, key="max_depth")
                    else:
                        max_depth = st.slider("ƒê·ªô s√¢u t·ªëi ƒëa:", 3, 20, step=1, key="max_depth")
                    
                    learning_rate = st.slider("Learning rate:", 0.01, 0.3, step=0.01, key="lr")
                    subsample = st.slider("Subsample:", 0.5, 1.0, step=0.1, key="subsample")
                    
                    # Early Stopping Configuration
                    st.markdown("---")
                    st.markdown("##### ‚èπÔ∏è Early Stopping")
                    
                    # Check if validation set exists
                    has_validation = st.session_state.get('valid_data') is not None
                    
                    if has_validation:
                        enable_early_stopping = st.checkbox(
                            "üõë B·∫≠t Early Stopping", 
                            value=True,
                            key="enable_early_stopping",
                            help="T·ª± ƒë·ªông d·ª´ng training khi validation metrics kh√¥ng c·∫£i thi·ªán sau N v√≤ng"
                        )
                        
                        if enable_early_stopping:
                            early_stopping_rounds = st.slider(
                                "S·ªë v√≤ng ch·ªù (patience):",
                                min_value=5, max_value=50, value=10, step=5,
                                key="early_stopping_rounds",
                                help="D·ª´ng training n·∫øu kh√¥ng c√≥ c·∫£i thi·ªán sau s·ªë v√≤ng n√†y"
                            )
                            params['early_stopping_rounds'] = early_stopping_rounds
                        else:
                            params['early_stopping_rounds'] = None
                    else:
                        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ t·∫≠p Validation. ƒê·ªÉ d√πng Early Stopping, h√£y chia d·ªØ li·ªáu Train/Validation/Test ·ªü b∆∞·ªõc Feature Engineering.")
                        params['early_stopping_rounds'] = None
                    
                    params['n_estimators'] = n_estimators
                    params['max_depth'] = max_depth
                    params['learning_rate'] = learning_rate
                    params['subsample'] = subsample
                
                elif model_type == "Stacking Ensemble":
                    st.markdown("""
                    <div style="background-color: #1e3a5f; padding: 1rem; border-radius: 8px; margin-bottom: 1rem; border-left: 4px solid #667eea;">
                        <p style="margin: 0; font-size: 0.9rem;">
                            üí° <strong>Stacking Ensemble</strong> k·∫øt h·ª£p nhi·ªÅu model ƒë·ªÉ t·∫°o ra predictions m·∫°nh h∆°n.
                            <br>D·ª±a tr√™n paper: <em>"Credit-Risk-Scoring: A Stacking Generalization Approach"</em>
                        </p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    st.markdown("##### üîß Base Models (Level 0)")
                    st.caption("Ch·ªçn 2-3 model c∆° b·∫£n ƒë·ªÉ x√¢y d·ª±ng Stacking")
                    
                    # Base model options with descriptions
                    base_model_options = {
                        'LR': 'Logistic Regression',
                        'DT': 'Decision Tree',
                        'SVM': 'Support Vector Machine',
                        'KNN': 'K-Nearest Neighbors',
                        'RF': 'Random Forest',
                        'GB': 'Gradient Boosting'
                    }
                    
                    # Initialize session state for stacking if not exists
                    if 'stacking_base_models' not in st.session_state:
                        st.session_state.stacking_base_models = ['LR', 'DT']
                    if 'stacking_meta_model' not in st.session_state:
                        st.session_state.stacking_meta_model = 'Random Forest'
                    
                    selected_base_models = st.multiselect(
                        "Ch·ªçn Base Models (2-3):",
                        options=list(base_model_options.keys()),
                        default=st.session_state.stacking_base_models,
                        format_func=lambda x: f"{x} - {base_model_options[x]}",
                        max_selections=3,
                        key="stacking_base_select"
                    )
                    
                    # Validate minimum 2 models
                    if len(selected_base_models) < 2:
                        st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn √≠t nh·∫•t 2 Base Models")
                    
                    st.session_state.stacking_base_models = selected_base_models
                    
                    st.markdown("##### üéØ Meta Model (Level 1)")
                    st.caption("Model s·∫Ω h·ªçc c√°ch k·∫øt h·ª£p predictions t·ª´ Base Models")
                    
                    meta_model_options = ["Random Forest", "Logistic Regression", "XGBoost"]
                    meta_model = st.selectbox(
                        "Meta Model (Final Estimator):",
                        options=meta_model_options,
                        index=meta_model_options.index(st.session_state.stacking_meta_model),
                        key="stacking_meta_select",
                        help="Random Forest ƒë∆∞·ª£c khuy·∫øn ngh·ªã (nh∆∞ trong paper g·ªëc)"
                    )
                    st.session_state.stacking_meta_model = meta_model
                    
                    # Initialize manual params dictionaries
                    manual_base_params = {}
                    manual_meta_params = {}

                    # ================== MANUAL CONFIGURATION SECTION ==================
                    st.markdown("##### ‚öôÔ∏è C·∫•u H√¨nh Tham S·ªë Chi Ti·∫øt")
                    st.caption("T√πy ch·ªânh tham s·ªë cho t·ª´ng model trong Stacking")

                    # 1. Base Models Configuration
                    st.markdown("###### Base Models:")
                    for model_key in selected_base_models:
                        model_name = base_model_options.get(model_key, model_key)
                        
                        # Default values
                        defaults = {}
                        if model_key == 'LR': defaults = {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}
                        if model_key == 'DT': defaults = {'max_depth': 10, 'min_samples_split': 2, 'criterion': 'gini', 'min_samples_leaf': 1}
                        if model_key == 'SVM': defaults = {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale'}
                        if model_key == 'KNN': defaults = {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'euclidean'}
                        if model_key == 'RF': defaults = {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}
                        if model_key == 'GB': defaults = {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'subsample': 1.0}

                        # Check if we have applied params from tuning (session state) to override defaults
                        if f'stack_man_{model_key}_C' not in st.session_state and 'stacking_applied_params' in st.session_state:
                             # Logic to pre-fill widgets if 'Apply' was clicked previously (optional helper if needed, 
                             # currently we rely on widget keys being updated by Ag-Grid or manual entry, 
                             # but here we set defaults if key doesn't exist yet)
                             pass

                        with st.expander(f"üîß {model_key} - {model_name}", expanded=False):
                            if model_key == 'LR':
                                k_c = f"stack_man_{model_key}_C"
                                k_pen = f"stack_man_{model_key}_penalty"
                                k_sol = f"stack_man_{model_key}_solver"
                                
                                c_val = st.number_input(f"C ({model_key})", min_value=0.001, max_value=100.0, value=st.session_state.get(k_c, defaults['C']), step=0.1, key=k_c)
                                penalty = st.selectbox(f"Penalty ({model_key})", ["l2", "l1", "elasticnet", "none"], index=["l2", "l1", "elasticnet", "none"].index(st.session_state.get(k_pen, defaults['penalty'])), key=k_pen)
                                solver = st.selectbox(f"Solver ({model_key})", ["lbfgs", "liblinear", "saga"], index=["lbfgs", "liblinear", "saga"].index(st.session_state.get(k_sol, defaults['solver'])), key=k_sol)
                                manual_base_params[model_key] = {'C': c_val, 'penalty': penalty, 'solver': solver}
                            
                            elif model_key == 'DT':
                                k_depth = f"stack_man_{model_key}_max_depth"
                                k_split = f"stack_man_{model_key}_min_samples_split"
                                k_leaf = f"stack_man_{model_key}_min_samples_leaf"
                                k_crit = f"stack_man_{model_key}_criterion"
                                
                                max_depth = st.number_input(f"Max Depth ({model_key})", min_value=1, max_value=100, value=st.session_state.get(k_depth, defaults['max_depth']), step=1, key=k_depth)
                                min_split = st.number_input(f"Min Samples Split ({model_key})", min_value=2, max_value=20, value=st.session_state.get(k_split, defaults['min_samples_split']), step=1, key=k_split)
                                min_leaf = st.number_input(f"Min Samples Leaf ({model_key})", min_value=1, max_value=20, value=st.session_state.get(k_leaf, defaults['min_samples_leaf']), step=1, key=k_leaf)
                                criterion = st.selectbox(f"Criterion ({model_key})", ["gini", "entropy"], index=["gini", "entropy"].index(st.session_state.get(k_crit, defaults['criterion'])), key=k_crit)
                                manual_base_params[model_key] = {'max_depth': max_depth, 'min_samples_split': min_split, 'min_samples_leaf': min_leaf, 'criterion': criterion}

                            elif model_key == 'SVM':
                                k_c = f"stack_man_{model_key}_C"
                                k_kern = f"stack_man_{model_key}_kernel"
                                k_gam = f"stack_man_{model_key}_gamma"
                                
                                c_val = st.number_input(f"C ({model_key})", min_value=0.1, max_value=50.0, value=float(st.session_state.get(k_c, defaults['C'])), step=0.1, key=k_c)
                                kernel = st.selectbox(f"Kernel ({model_key})", ["rbf", "linear", "poly", "sigmoid"], index=["rbf", "linear", "poly", "sigmoid"].index(st.session_state.get(k_kern, defaults['kernel'])), key=k_kern)
                                gamma = st.selectbox(f"Gamma ({model_key})", ["scale", "auto"], index=["scale", "auto"].index(st.session_state.get(k_gam, defaults['gamma'])), key=k_gam)
                                manual_base_params[model_key] = {'C': c_val, 'kernel': kernel, 'gamma': gamma}

                            elif model_key == 'KNN':
                                k_n = f"stack_man_{model_key}_n_neighbors"
                                k_w = f"stack_man_{model_key}_weights"
                                k_m = f"stack_man_{model_key}_metric"
                                
                                n_neighbors = st.number_input(f"N Neighbors ({model_key})", min_value=1, max_value=50, value=st.session_state.get(k_n, defaults['n_neighbors']), step=1, key=k_n)
                                weights = st.selectbox(f"Weights ({model_key})", ["uniform", "distance"], index=["uniform", "distance"].index(st.session_state.get(k_w, defaults['weights'])), key=k_w)
                                metric = st.selectbox(f"Metric ({model_key})", ["euclidean", "manhattan", "minkowski"], index=["euclidean", "manhattan", "minkowski"].index(st.session_state.get(k_m, 'euclidean')), key=k_m)
                                manual_base_params[model_key] = {'n_neighbors': n_neighbors, 'weights': weights, 'metric': metric}

                            elif model_key == 'RF':
                                k_n = f"stack_man_{model_key}_n_estimators"
                                k_d = f"stack_man_{model_key}_max_depth"
                                k_s = f"stack_man_{model_key}_min_samples_split"
                                k_l = f"stack_man_{model_key}_min_samples_leaf"
                                
                                n_estimators = st.number_input(f"N Estimators ({model_key})", min_value=10, max_value=500, value=st.session_state.get(k_n, defaults['n_estimators']), step=10, key=k_n)
                                max_depth_val = st.number_input(f"Max Depth ({model_key})", min_value=1, max_value=50, value=st.session_state.get(k_d, defaults['max_depth']), step=1, key=k_d)
                                min_split = st.number_input(f"Min Samples Split ({model_key})", min_value=2, max_value=20, value=st.session_state.get(k_s, 2), step=1, key=k_s)
                                min_leaf = st.number_input(f"Min Samples Leaf ({model_key})", min_value=1, max_value=20, value=st.session_state.get(k_l, 1), step=1, key=k_l)
                                manual_base_params[model_key] = {'n_estimators': n_estimators, 'max_depth': max_depth_val, 'min_samples_split': min_split, 'min_samples_leaf': min_leaf}

                            elif model_key == 'GB':
                                k_n = f"stack_man_{model_key}_n_estimators"
                                k_lr = f"stack_man_{model_key}_learning_rate"
                                k_d = f"stack_man_{model_key}_max_depth"
                                k_s = f"stack_man_{model_key}_min_samples_split"
                                k_sub = f"stack_man_{model_key}_subsample"
                                
                                n_estimators = st.number_input(f"N Estimators ({model_key})", min_value=10, max_value=500, value=st.session_state.get(k_n, defaults['n_estimators']), step=10, key=k_n)
                                learning_rate = st.number_input(f"Learning Rate ({model_key})", min_value=0.01, max_value=1.0, value=float(st.session_state.get(k_lr, defaults['learning_rate'])), step=0.01, format="%.2f", key=k_lr)
                                max_depth = st.number_input(f"Max Depth ({model_key})", min_value=1, max_value=20, value=st.session_state.get(k_d, defaults['max_depth']), step=1, key=k_d)
                                min_split = st.number_input(f"Min Samples Split ({model_key})", min_value=2, max_value=20, value=st.session_state.get(k_s, 2), step=1, key=k_s)
                                subsample = st.number_input(f"Subsample ({model_key})", min_value=0.1, max_value=1.0, value=float(st.session_state.get(k_sub, 0.8)), step=0.05, key=k_sub)
                                manual_base_params[model_key] = {'n_estimators': n_estimators, 'learning_rate': learning_rate, 'max_depth': max_depth, 'min_samples_split': min_split, 'subsample': subsample}

                    # 2. Meta Model Configuration
                    st.markdown("###### Meta Model:")
                    with st.expander(f"üéØ {meta_model} (Meta)", expanded=False):
                        if meta_model == "Random Forest":
                            k_n = "stack_man_meta_n_estimators"
                            k_d = "stack_man_meta_max_depth"
                            k_s = "stack_man_meta_min_samples_split"
                            
                            n_est = st.slider("S·ªë c√¢y (n_estimators):", min_value=10, max_value=500, value=st.session_state.get(k_n, 100), step=10, key=k_n)
                            m_depth = st.slider("ƒê·ªô s√¢u t·ªëi ƒëa:", min_value=1, max_value=50, value=st.session_state.get(k_d, 10), step=1, key=k_d)
                            min_split = st.slider("Min samples split:", min_value=2, max_value=20, value=st.session_state.get(k_s, 2), step=1, key=k_s)
                            manual_meta_params = {'n_estimators': n_est, 'max_depth': m_depth, 'min_samples_split': min_split}
                            
                        elif meta_model == "Logistic Regression":
                            k_c = "stack_man_meta_C"
                            k_iter = "stack_man_meta_max_iter"
                            
                            c_val = st.slider("C (Regularization):", min_value=0.01, max_value=10.0, value=float(st.session_state.get(k_c, 1.0)), step=0.01, key=k_c)
                            max_iter = st.number_input("Max iterations:", min_value=100, max_value=5000, value=st.session_state.get(k_iter, 200), step=100, key=k_iter)
                            manual_meta_params = {'C': c_val, 'max_iter': max_iter}
                            
                        elif meta_model == "XGBoost":
                            k_n = "stack_man_meta_n_estimators"
                            k_d = "stack_man_meta_max_depth"
                            k_lr = "stack_man_meta_learning_rate"
                            k_sub = "stack_man_meta_subsample"
                            k_child = "stack_man_meta_min_child_weight"
                            
                            n_est = st.slider("S·ªë c√¢y (n_estimators):", min_value=10, max_value=500, value=st.session_state.get(k_n, 100), step=10, key=k_n)
                            m_depth = st.slider("ƒê·ªô s√¢u t·ªëi ƒëa:", min_value=1, max_value=20, value=st.session_state.get(k_d, 6), step=1, key=k_d)
                            # Update learning rate step to 0.01 to allow fine values like 0.05
                            lr = st.slider("Learning rate:", min_value=0.01, max_value=1.0, value=float(st.session_state.get(k_lr, 0.1)), step=0.01, key=k_lr)
                            subsample = st.slider("Subsample:", min_value=0.1, max_value=1.0, value=float(st.session_state.get(k_sub, 0.8)), step=0.05, key=k_sub)
                            # Add min_child_weight slider as per tuning param
                            min_child = st.slider("Min Child Weight:", min_value=1, max_value=10, value=st.session_state.get(k_child, 1), step=1, key=k_child)
                            
                            manual_meta_params = {'n_estimators': n_est, 'max_depth': m_depth, 'learning_rate': lr, 'subsample': subsample, 'min_child_weight': min_child}
                    
                    st.info("üí° B·∫°n c√≥ th·ªÉ ch·ªânh s·ª≠a tham s·ªë th·ªß c√¥ng ·ªü tr√™n ho·∫∑c auto-tune b√™n tab 'Hyperparameter Tuning'.")

                    # Store in params for training
                    params['enable_tuning'] = False
                    params['base_models'] = selected_base_models
                    params['meta_model'] = meta_model
                    # Pass manual params to backend
                    params['base_model_params'] = manual_base_params
                    params['meta_model_params'] = manual_meta_params
                
                st.markdown("---")
                
                # Train button - check flag immediately to prevent double render
                if not st.session_state.get('_training_in_progress', False):
                    if st.button("üöÄ Hu·∫•n Luy·ªán M√¥ H√¨nh", type="primary", key="train_model_btn", width='stretch'):
                        # L∆∞u params v√†o session state v√† set flag
                        st.session_state._training_model_type = model_type
                        st.session_state._training_params = params
                        st.session_state._training_in_progress = True
                        # Preserve tuning method selection during training
                        if 'tuning_method' in st.session_state:
                            st.session_state._last_tuning_method = st.session_state.tuning_method
                        st.rerun()
            
            with col2:
                st.markdown("#### üìã Th√¥ng Tin Hu·∫•n Luy·ªán")
                
                # Training info panel
                st.markdown("""
                <div style="background-color: #262730; padding: 1.5rem; border-radius: 10px; margin-bottom: 1rem;">
                    <h4 style="margin-top: 0; color: #667eea;">üí° H∆∞·ªõng D·∫´n</h4>
                    <ul style="margin-bottom: 0;">
                        <li><strong>Logistic Regression</strong>: Nhanh, d·ªÖ gi·∫£i th√≠ch, ph√π h·ª£p v·ªõi d·ªØ li·ªáu tuy·∫øn t√≠nh</li>
                        <li><strong>Random Forest</strong>: M·∫°nh m·∫Ω, ch·ªëng overfitting t·ªët</li>
                        <li><strong>XGBoost/LightGBM</strong>: Hi·ªáu su·∫•t cao, th∆∞·ªùng cho k·∫øt qu·∫£ t·ªët nh·∫•t</li>
                        <li><strong>CatBoost</strong>: T·ªët v·ªõi bi·∫øn ph√¢n lo·∫°i, √≠t c·∫ßn ti·ªÅn x·ª≠ l√Ω</li>
                        <li><strong>Stacking Ensemble</strong>: K·∫øt h·ª£p nhi·ªÅu m√¥ h√¨nh base v·ªõi meta-learner, th∆∞·ªùng ƒë·∫°t hi·ªáu su·∫•t cao nh·∫•t</li>
                    </ul>
                </div>
                """, unsafe_allow_html=True)
                
                # Current configuration summary - use selected_model_name if available
                current_model_display = st.session_state.get('selected_model_name', None)
                if current_model_display is None and st.session_state.model is not None:
                    current_model_display = st.session_state.get('model_type_select', 'Unknown')
                
                if st.session_state.model is not None and current_model_display:
                    st.markdown("#### ‚úÖ M√¥ H√¨nh Hi·ªán T·∫°i")
                    
                    st.markdown(f"""
                    <div style="background: linear-gradient(135deg, #1f2937 0%, #345f9c 100%); 
                                padding: 1.5rem; border-radius: 10px;">
                        <h3 style="margin: 0; color: white;">{current_model_display}</h3>
                        <p style="margin: 0.5rem 0 0 0; color: rgba(255,255,255,0.9);">
                            ƒê√£ hu·∫•n luy·ªán v·ªõi {len(st.session_state.selected_features)} features
                        </p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    st.markdown("<br>", unsafe_allow_html=True)
                    
                    # Quick metrics
                    metrics = st.session_state.model_metrics
                    mcol1, mcol2, mcol3 = st.columns(3)
                    
                    with mcol1:
                        st.metric("Accuracy", f"{metrics['accuracy']:.3f}")
                    with mcol2:
                        st.metric("Precision", f"{metrics['precision']:.3f}")
                    with mcol3:
                        st.metric("AUC", f"{metrics['auc']:.3f}")
                else:
                    st.info("‚è≥ Ch∆∞a hu·∫•n luy·ªán m√¥ h√¨nh n√†o")
                
                # Additional options
                st.markdown("#### ‚öôÔ∏è T√πy Ch·ªçn N√¢ng Cao")
                
                with st.expander("Cross-Validation"):
                    cv_folds = st.slider("S·ªë folds:", 3, 10, 5, key="cv_folds")
                    if st.button("üîÑ Ch·∫°y Cross-Validation", key="run_cv"):
                        try:
                            with st.spinner(f"ƒêang ch·∫°y Cross-Validation v·ªõi {cv_folds} folds..."):
                                # Prepare data
                                target_col = st.session_state.target_column
                                features = st.session_state.selected_features
                                
                                X = st.session_state.train_data[features]
                                y = st.session_state.train_data[target_col]
                                
                                # Get current model type from selectbox
                                current_model_type = st.session_state.get('model_type_select', 'Logistic Regression')
                                
                                # Collect parameters based on model type
                                params = {}
                                if current_model_type == "Logistic Regression":
                                    params['C'] = st.session_state.get('lr_c', 1.0)
                                    params['max_iter'] = st.session_state.get('lr_iter', 200)
                                elif current_model_type == "Random Forest":
                                    params['n_estimators'] = st.session_state.get('n_trees', 100)
                                    params['max_depth'] = st.session_state.get('max_depth', 10)
                                elif current_model_type in ["XGBoost", "LightGBM", "CatBoost", "Gradient Boosting"]:
                                    params['n_estimators'] = st.session_state.get('n_trees', 100)
                                    params['max_depth'] = st.session_state.get('max_depth', 6)
                                    params['learning_rate'] = st.session_state.get('lr', 0.1)
                                    params['subsample'] = st.session_state.get('subsample', 0.8)
                                
                                # Import backend
                                from backend.models.trainer import cross_validate_model
                                
                                # Run cross-validation
                                cv_results = cross_validate_model(X, y, current_model_type, params, cv_folds)
                                
                                # Save to session state
                                st.session_state.cv_results = cv_results
                                
                                # Display results
                                st.success(f"‚úÖ Cross-Validation ho√†n th√†nh!")
                                
                                # Show metrics
                                cv_col1, cv_col2 = st.columns(2)
                                with cv_col1:
                                    st.metric("AUC", f"{cv_results['auc']['mean']:.3f} (+/- {cv_results['auc']['std']:.3f})")
                                    st.metric("Accuracy", f"{cv_results['accuracy']['mean']:.3f} (+/- {cv_results['accuracy']['std']:.3f})")
                                with cv_col2:
                                    st.metric("F1-Score", f"{cv_results['f1']['mean']:.3f} (+/- {cv_results['f1']['std']:.3f})")
                                    st.metric("Precision", f"{cv_results['precision']['mean']:.3f} (+/- {cv_results['precision']['std']:.3f})")
                                
                                # Show fold details (no nested expander)
                                st.markdown("##### üìã Chi ti·∫øt t·ª´ng Fold")
                                fold_df = pd.DataFrame({
                                    'Fold': [f"Fold {i+1}" for i in range(cv_folds)],
                                    'Accuracy': cv_results['accuracy']['scores'],
                                    'Precision': cv_results['precision']['scores'],
                                    'Recall': cv_results['recall']['scores'],
                                    'F1': cv_results['f1']['scores'],
                                    'AUC': cv_results['auc']['scores']
                                })
                                st.dataframe(fold_df.style.format({
                                    'Accuracy': '{:.3f}',
                                    'Precision': '{:.3f}',
                                    'Recall': '{:.3f}',
                                    'F1': '{:.3f}',
                                    'AUC': '{:.3f}'
                                }).background_gradient(subset=['AUC'], cmap='RdYlGn'), width='stretch')
                        except Exception as e:
                            st.error(f"‚ùå L·ªói khi ch·∫°y Cross-Validation: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc())
                
                with st.expander("Hyperparameter Tuning"):
                    # Get current model type
                    current_model_type = st.session_state.get('model_type_select', 'Logistic Regression')
                    
                    # Special UI for Stacking Ensemble
                    if current_model_type == "Stacking Ensemble":
                        st.markdown("""
                        <div style="background-color: #1e3a5f; padding: 0.8rem; border-radius: 8px; margin-bottom: 1rem; border-left: 4px solid #667eea;">
                            <p style="margin: 0; font-size: 0.9rem;">
                                üî¨ <strong>OOF Hyperparameter Tuning</strong>: Tune t·ª´ng base model + meta model v·ªõi Out-of-Fold ƒë·ªÉ tr√°nh data leakage.
                            </p>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        # Tuning method and CV folds
                        tuning_col1, tuning_col2 = st.columns(2)
                        
                        with tuning_col1:
                            stacking_tuning_method = st.selectbox(
                                "Ph∆∞∆°ng ph√°p:",
                                ["Grid Search", "Random Search"],
                                key="stacking_hp_tuning_method"
                            )
                        
                        with tuning_col2:
                            stacking_cv_folds = st.slider(
                                "S·ªë folds cho CV:",
                                min_value=3, max_value=10, value=5,
                                key="stacking_hp_cv_folds"
                            )
                        
                        # Get selected base models from session state
                        selected_base_models = st.session_state.get('stacking_base_models', ['LR', 'DT'])
                        meta_model = st.session_state.get('stacking_meta_model', 'Random Forest')
                        
                        base_model_options = {
                            'LR': 'Logistic Regression',
                            'DT': 'Decision Tree',
                            'SVM': 'Support Vector Machine',
                            'KNN': 'K-Nearest Neighbors',
                            'RF': 'Random Forest',
                            'GB': 'Gradient Boosting'
                        }
                        
                        st.markdown("##### ‚öôÔ∏è Tham S·ªë Base Models")
                        
                        stacking_base_params = {}
                        
                        # Create param grids for each selected base model
                        for model_key in selected_base_models:
                            model_name = base_model_options.get(model_key, model_key)
                            
                            with st.expander(f"üîß {model_key} - {model_name}", expanded=False):
                                if model_key == 'LR':
                                    st.markdown("**Logistic Regression**")
                                    c_values = st.multiselect("C (Regularization):", [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0], default=[0.1, 1.0, 10.0], key=f"hp_{model_key}_c")
                                    penalty = st.multiselect("Penalty:", ["l1", "l2", "elasticnet", "none"], default=["l2"], key=f"hp_{model_key}_penalty")
                                    solver = st.multiselect("Solver:", ["lbfgs", "liblinear", "saga"], default=["lbfgs", "saga"], key=f"hp_{model_key}_solver")
                                    stacking_base_params['LR'] = {
                                        'C': c_values if c_values else [1.0],
                                        'penalty': penalty if penalty else ['l2'],
                                        'solver': solver if solver else ['lbfgs']
                                    }
                                    
                                elif model_key == 'DT':
                                    st.markdown("**Decision Tree**")
                                    depth = st.multiselect("Max Depth:", [3, 5, 7, 10, 12, 15, 20, None], default=[5, 10, 15], key=f"hp_{model_key}_depth")
                                    min_split = st.multiselect("Min Samples Split:", [2, 5, 10, 20], default=[2, 5, 10], key=f"hp_{model_key}_min_split")
                                    min_leaf = st.multiselect("Min Samples Leaf:", [1, 2, 4, 8], default=[1, 2, 4], key=f"hp_{model_key}_min_leaf")
                                    criterion = st.multiselect("Criterion:", ["gini", "entropy"], default=["gini", "entropy"], key=f"hp_{model_key}_criterion")
                                    stacking_base_params['DT'] = {
                                        'max_depth': depth if depth else [10],
                                        'min_samples_split': min_split if min_split else [2],
                                        'min_samples_leaf': min_leaf if min_leaf else [1],
                                        'criterion': criterion if criterion else ['gini']
                                    }
                                    
                                elif model_key == 'SVM':
                                    st.markdown("**Support Vector Machine**")
                                    c_values = st.multiselect("C:", [0.1, 0.5, 1.0, 2.0, 5.0, 10.0], default=[0.1, 1.0, 10.0], key=f"hp_{model_key}_c")
                                    kernel = st.multiselect("Kernel:", ["rbf", "linear", "poly", "sigmoid"], default=["rbf", "linear"], key=f"hp_{model_key}_kernel")
                                    gamma = st.multiselect("Gamma:", ["scale", "auto", 0.001, 0.01, 0.1, 1], default=["scale", "auto"], key=f"hp_{model_key}_gamma")
                                    stacking_base_params['SVM'] = {
                                        'C': c_values if c_values else [1.0],
                                        'kernel': kernel if kernel else ['rbf'],
                                        'gamma': gamma if gamma else ['scale']
                                    }
                                    
                                elif model_key == 'KNN':
                                    st.markdown("**K-Nearest Neighbors**")
                                    k_values = st.multiselect("N Neighbors:", [3, 5, 7, 9, 11, 15, 21], default=[3, 5, 7], key=f"hp_{model_key}_k")
                                    weights = st.multiselect("Weights:", ["uniform", "distance"], default=["uniform", "distance"], key=f"hp_{model_key}_weights")
                                    metric = st.multiselect("Metric:", ["euclidean", "manhattan", "minkowski"], default=["euclidean", "manhattan"], key=f"hp_{model_key}_metric")
                                    stacking_base_params['KNN'] = {
                                        'n_neighbors': k_values if k_values else [5],
                                        'weights': weights if weights else ['uniform'],
                                        'metric': metric if metric else ['euclidean']
                                    }
                                    
                                elif model_key == 'RF':
                                    st.markdown("**Random Forest**")
                                    n_trees = st.multiselect("N Estimators:", [50, 100, 150, 200, 300], default=[100, 200], key=f"hp_{model_key}_trees")
                                    depth = st.multiselect("Max Depth:", [5, 10, 15, 20, None], default=[10, 15, None], key=f"hp_{model_key}_depth")
                                    min_split = st.multiselect("Min Samples Split:", [2, 5, 10], default=[2, 5], key=f"hp_{model_key}_min_split")
                                    min_leaf = st.multiselect("Min Samples Leaf:", [1, 2, 4], default=[1, 2], key=f"hp_{model_key}_min_leaf")
                                    stacking_base_params['RF'] = {
                                        'n_estimators': n_trees if n_trees else [100],
                                        'max_depth': depth if depth else [10],
                                        'min_samples_split': min_split if min_split else [2],
                                        'min_samples_leaf': min_leaf if min_leaf else [1]
                                    }
                                    
                                elif model_key == 'GB':
                                    st.markdown("**Gradient Boosting**")
                                    n_trees = st.multiselect("N Estimators:", [50, 100, 150, 200], default=[100, 150], key=f"hp_{model_key}_trees")
                                    lr_val = st.multiselect("Learning Rate:", [0.01, 0.05, 0.1, 0.2, 0.3], default=[0.05, 0.1], key=f"hp_{model_key}_lr")
                                    depth = st.multiselect("Max Depth:", [3, 5, 7, 10], default=[3, 5], key=f"hp_{model_key}_depth")
                                    min_split = st.multiselect("Min Samples Split:", [2, 5, 10], default=[2, 5], key=f"hp_{model_key}_min_split")
                                    subsample = st.multiselect("Subsample:", [0.6, 0.7, 0.8, 0.9, 1.0], default=[0.8, 1.0], key=f"hp_{model_key}_subsample")
                                    stacking_base_params['GB'] = {
                                        'n_estimators': n_trees if n_trees else [100],
                                        'learning_rate': lr_val if lr_val else [0.1],
                                        'max_depth': depth if depth else [3],
                                        'min_samples_split': min_split if min_split else [2],
                                        'subsample': subsample if subsample else [1.0]
                                    }
                        
                        st.markdown("##### üéØ Tham S·ªë Meta Model")
                        
                        stacking_meta_params = {}
                        
                        with st.expander(f"üéØ Meta Model - {meta_model}", expanded=False):
                            if meta_model == "Random Forest":
                                st.markdown("**Random Forest (Meta)**")
                                meta_trees = st.multiselect("N Estimators:", [50, 100, 150, 200, 300], default=[100, 200], key="hp_meta_rf_trees")
                                meta_depth = st.multiselect("Max Depth:", [5, 10, 15, 20, None], default=[10, None], key="hp_meta_rf_depth")
                                meta_min_split = st.multiselect("Min Samples Split:", [2, 5, 10], default=[2, 5], key="hp_meta_rf_min_split")
                                stacking_meta_params = {
                                    'n_estimators': meta_trees if meta_trees else [100],
                                    'max_depth': meta_depth if meta_depth else [None],
                                    'min_samples_split': meta_min_split if meta_min_split else [2]
                                }
                                
                            elif meta_model == "Logistic Regression":
                                st.markdown("**Logistic Regression (Meta)**")
                                meta_c = st.multiselect("C:", [0.01, 0.1, 0.5, 1.0, 5.0, 10.0], default=[0.1, 1.0, 10.0], key="hp_meta_lr_c")
                                meta_penalty = st.multiselect("Penalty:", ["l1", "l2", "none"], default=["l2"], key="hp_meta_lr_penalty")
                                stacking_meta_params = {
                                    'C': meta_c if meta_c else [1.0],
                                    'penalty': meta_penalty if meta_penalty else ['l2']
                                }
                                
                            elif meta_model == "XGBoost":
                                st.markdown("**XGBoost (Meta)**")
                                meta_trees = st.multiselect("N Estimators:", [50, 100, 150, 200], default=[100, 150], key="hp_meta_xgb_trees")
                                meta_lr = st.multiselect("Learning Rate:", [0.01, 0.05, 0.1, 0.2], default=[0.05, 0.1], key="hp_meta_xgb_lr")
                                meta_depth = st.multiselect("Max Depth:", [3, 4, 5, 6, 8], default=[5, 6], key="hp_meta_xgb_depth")
                                meta_child = st.multiselect("Min Child Weight:", [1, 3, 5], default=[1, 3], key="hp_meta_xgb_child")
                                meta_subsample = st.multiselect("Subsample:", [0.6, 0.8, 1.0], default=[0.8, 1.0], key="hp_meta_xgb_subsample")
                                stacking_meta_params = {
                                    'n_estimators': meta_trees if meta_trees else [100],
                                    'learning_rate': meta_lr if meta_lr else [0.1],
                                    'max_depth': meta_depth if meta_depth else [6],
                                    'min_child_weight': meta_child if meta_child else [1],
                                    'subsample': meta_subsample if meta_subsample else [1.0]
                                }
                        
                        if st.button("üîç T√¨m Tham S·ªë T·ªët Nh·∫•t (OOF)", key="stacking_tune_btn"):
                            try:
                                with st.spinner("ƒêang ch·∫°y OOF Hyperparameter Tuning... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)"):
                                    target_col = st.session_state.target_column
                                    features = st.session_state.selected_features
                                    
                                    X_train = st.session_state.train_data[features]
                                    y_train = st.session_state.train_data[target_col]
                                    
                                    if 'test_data' in st.session_state and st.session_state.test_data is not None:
                                        X_test = st.session_state.test_data[features]
                                        y_test = st.session_state.test_data[target_col]
                                    else:
                                        from sklearn.model_selection import train_test_split
                                        X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
                                    
                                    from backend.models.trainer import tune_stacking_with_oof
                                    
                                    model, metrics, tuning_info = tune_stacking_with_oof(
                                        X_train, y_train, X_test, y_test,
                                        base_models_config=stacking_base_params,
                                        meta_model=meta_model,
                                        tuning_method=stacking_tuning_method,
                                        n_folds=stacking_cv_folds,
                                        params={'meta_model_params': stacking_meta_params}
                                    )
                                    
                                    # Save to session state
                                    st.session_state.model = model
                                    st.session_state.model_metrics = metrics
                                    st.session_state.selected_model_name = f"Stacking ({stacking_tuning_method})"
                                    st.session_state.stacking_tuning_info = tuning_info
                                    
                                    # Add to history
                                    import datetime
                                    history_entry = {
                                        'Model': f"Stacking (Tuned)",
                                        'Accuracy': metrics['accuracy'],
                                        'Precision': metrics['precision'],
                                        'Recall': metrics['recall'],
                                        'F1-Score': metrics['f1'],
                                        'AUC': metrics['auc'],
                                        'Timestamp': datetime.datetime.now().strftime("%H:%M:%S"),
                                        'Params': str(tuning_info.get('best_params_per_model', {}))
                                    }
                                    st.session_state.model_history.append(history_entry)
                                    
                                st.success("‚úÖ OOF Tuning ho√†n th√†nh!")
                                st.rerun()
                                    
                            except Exception as e:
                                st.error(f"‚ùå L·ªói: {str(e)}")
                                import traceback
                                st.code(traceback.format_exc())
                        
                        # Display stacking tuning results if exists
                        if 'stacking_tuning_info' in st.session_state and st.session_state.stacking_tuning_info:
                            tuning_info = st.session_state.stacking_tuning_info
                            
                            st.markdown("##### üèÜ K·∫øt Qu·∫£ Tuning")
                            
                            st.markdown(f"**Ph∆∞∆°ng ph√°p:** {tuning_info.get('tuning_method', 'N/A')}")
                            st.markdown(f"**S·ªë folds:** {tuning_info.get('n_folds', 5)}")
                            
                            st.markdown("**Best Params per Model:**")
                            for model_key, params in tuning_info.get('tuning_results', {}).items():
                                best_score = params.get('best_score', 'N/A')
                                best_params = params.get('best_params', {})
                                score_str = f"{best_score:.4f}" if isinstance(best_score, float) else str(best_score)
                                st.markdown(f"- **{model_key}**: AUC = {score_str}")
                                st.code(str(best_params))
                            
                            # Define update callback
                            def update_stacking_widgets(results):
                                cnt = 0
                                for model_key, result in results.items():
                                    b_params = result.get('best_params', {})
                                    
                                    if model_key == 'META_MODEL':
                                        # Meta Params
                                        for p_name, p_val in b_params.items():
                                            w_key = f"stack_man_meta_{p_name}"
                                            st.session_state[w_key] = p_val
                                            cnt += 1
                                    else:
                                        # Base Params
                                        for p_name, p_val in b_params.items():
                                            w_key = f"stack_man_{model_key}_{p_name}"
                                            st.session_state[w_key] = p_val
                                            cnt += 1
                                
                                st.session_state.stacking_applied_params = results
                                # st.toast(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t {cnt} tham s·ªë t·ªëi ∆∞u!", icon="üéâ")

                            st.button(
                                "‚úÖ √Åp D·ª•ng Tham S·ªë T·ªët Nh·∫•t (Stacking)", 
                                key="apply_stacking_best_params",
                                on_click=update_stacking_widgets,
                                args=(tuning_info.get('tuning_results', {}),)
                            )
                            
                            # Trigger rerun if button was clicked (Streamlit reruns automatically on click, 
                            # but sometimes explicit check helps if we want to show immediate success message outside callback)
                            pass
                            
                            if st.session_state.get('stacking_applied_params') == tuning_info.get('tuning_results', {}):
                                # Just a visual indicator that it's applied
                                st.success("‚úÖ ƒê√£ √°p d·ª•ng tham s·ªë t·ªëi ∆∞u v√†o b·∫£ng c·∫•u h√¨nh!")
                    
                    else:
                        # Original UI for non-stacking models
                        # Define tuning methods list
                        tuning_methods = ["Grid Search", "Random Search", "Optuna (Bayesian)", "Bayesian Optimization"]
                        
                        # Get default index from saved tuning method
                        tuning_default_idx = 0
                        saved_method = st.session_state.get('_last_tuning_method', None)
                        if saved_method and saved_method in tuning_methods:
                            tuning_default_idx = tuning_methods.index(saved_method)
                        
                        tuning_method = st.selectbox(
                            "Ph∆∞∆°ng ph√°p:",
                            tuning_methods,
                            index=tuning_default_idx,
                            key="tuning_method"
                        )
                        
                        # Update saved method when user changes selection
                        st.session_state._last_tuning_method = tuning_method
                        
                        tuning_cv_folds = st.slider("S·ªë folds cho CV:", 3, 10, 5, key="tuning_cv_folds")
                        
                        # Show n_trials slider for Optuna/Bayesian
                        if "Optuna" in tuning_method or "Bayesian" in tuning_method:
                            n_trials = st.slider("S·ªë trials:", 20, 200, 50, key="n_trials")
                            st.caption("üí° Optuna s·ª≠ d·ª•ng TPE (Tree-structured Parzen Estimator) ƒë·ªÉ t·ªëi ∆∞u th√¥ng minh")
                    
                    if current_model_type != "Stacking Ensemble" and st.button("üîç T√¨m Tham S·ªë T·ªët Nh·∫•t", key="tune_params"):
                        try:
                            with st.spinner(f"ƒêang ch·∫°y {tuning_method}... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)"):
                                # Prepare data
                                target_col = st.session_state.target_column
                                features = st.session_state.selected_features
                                
                                X = st.session_state.train_data[features]
                                y = st.session_state.train_data[target_col]
                                
                                # Get current model type from selectbox
                                current_model_type = st.session_state.get('model_type_select', 'Logistic Regression')
                                
                                # Import backend
                                from backend.models.trainer import hyperparameter_tuning
                                
                                # Get n_trials for Optuna methods
                                n_trials_val = st.session_state.get('n_trials', 50) if ("Optuna" in tuning_method or "Bayesian" in tuning_method) else 50
                                
                                # Map UI method name to backend expected name
                                method_map = {
                                    "Grid Search": "Grid Search",
                                    "Random Search": "Random Search",
                                    "Optuna (Bayesian)": "Optuna",
                                    "Bayesian Optimization": "Bayesian Optimization"
                                }
                                backend_method = method_map.get(tuning_method, tuning_method)
                                
                                # Run hyperparameter tuning
                                tuning_results = hyperparameter_tuning(
                                    X, y, current_model_type, backend_method, tuning_cv_folds, n_trials_val
                                )
                                
                                # Save to session state and rerun to display results
                                st.session_state.tuning_results = tuning_results
                                # Save the tuning method to preserve selection after rerun
                                st.session_state._last_tuning_method = tuning_method
                            
                            st.rerun()
                                
                        except Exception as e:
                            st.error(f"‚ùå L·ªói khi ch·∫°y Hyperparameter Tuning: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc())
                    
                    # Display saved tuning results if exists
                    if 'tuning_results' in st.session_state and st.session_state.tuning_results is not None:
                        tuning_results = st.session_state.tuning_results
                        
                        st.success(f"‚úÖ ƒê√£ t√¨m ƒë∆∞·ª£c tham s·ªë t·ªët nh·∫•t!")
                        
                        st.markdown("##### üèÜ Tham S·ªë T·ªët Nh·∫•t")
                        
                        # Format best params for display with explanations
                        best_params_display = tuning_results['best_params'].copy()
                        current_model_type = st.session_state.get('model_type_select', 'Unknown')
                        
                        # Add explanation for special values
                        if 'max_depth' in best_params_display and best_params_display['max_depth'] == -1:
                            st.info(f"üí° **L∆∞u √Ω:** `max_depth = -1` trong {current_model_type} c√≥ nghƒ©a l√† **kh√¥ng gi·ªõi h·∫°n ƒë·ªô s√¢u** (c√¢y s·∫Ω ph√°t tri·ªÉn cho ƒë·∫øn khi c√°c leaf nodes ƒë·ªÅu pure ho·∫∑c ch·ª©a √≠t h∆°n min_samples_split samples). Khi √°p d·ª•ng, slider s·∫Ω ƒë∆∞·ª£c ƒë·∫∑t v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh (6).")
                        
                        st.json(best_params_display)
                        
                        st.metric("Best AUC Score", f"{tuning_results['best_score']:.4f}")
                        st.info(f"üìä ƒê√£ th·ª≠ {tuning_results['total_fits']} t·ªï h·ª£p tham s·ªë")
                        
                        # Show top 5 results (no nested expander)
                        st.markdown("##### üîù Top 5 T·ªï H·ª£p Tham S·ªë T·ªët Nh·∫•t")
                        for i, result in enumerate(tuning_results['top_results']):
                            st.markdown(f"**#{i+1}** - AUC: {result['mean_test_score']:.4f} (+/- {result['std_test_score']:.4f})")
                            st.code(str(result['params']))
                        
                        # Button to apply best params
                        if st.button("‚úÖ √Åp D·ª•ng Tham S·ªë T·ªët Nh·∫•t", key="apply_best_params"):
                            best_params = tuning_results['best_params']
                            st.session_state.best_tuned_params = best_params
                            
                            # Set flag to clear widget keys on next rerun
                            st.session_state.apply_tuned_params_flag = True
                            
                            # Clear tuning results after applying
                            st.session_state.tuning_results = None
                            st.success("‚úÖ ƒê√£ √°p d·ª•ng tham s·ªë t·ªët nh·∫•t!")
                            st.rerun()
    
    # Tab 2: Evaluation Results
    with tab2:
        st.markdown("### üìä K·∫øt Qu·∫£ ƒê√°nh Gi√° M√¥ H√¨nh")
        
        if st.session_state.model is None:
            st.warning("‚ö†Ô∏è Ch∆∞a c√≥ m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán. Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc.")
        else:
            metrics = st.session_state.model_metrics
            
            # Metrics overview
            st.markdown("#### üìà C√°c Ch·ªâ S·ªë ƒê√°nh Gi√°")
            
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                st.metric("Accuracy", f"{metrics['accuracy']:.3f}")
            with col2:
                st.metric("Precision", f"{metrics['precision']:.3f}")
            with col3:
                st.metric("Recall", f"{metrics['recall']:.3f}")
            with col4:
                st.metric("F1-Score", f"{metrics['f1']:.3f}")
            with col5:
                st.metric("AUC", f"{metrics['auc']:.3f}")
            
            # Early Stopping Info (if applicable)
            if metrics.get('early_stopped_iteration') is not None:
                st.success(f"‚èπÔ∏è **Early Stopping** ƒë√£ k√≠ch ho·∫°t t·∫°i v√≤ng l·∫∑p th·ª© **{metrics['early_stopped_iteration']}**")
            
            # Train vs Validation vs Test Metrics Comparison
            if metrics.get('train_metrics') is not None:
                st.markdown("---")
                st.markdown("#### üîç So S√°nh Train / Validation / Test (Ph√°t Hi·ªán Overfitting)")
                
                train_m = metrics.get('train_metrics', {})
                valid_m = metrics.get('valid_metrics')
                test_m = metrics.get('test_metrics', {})
                
                # Create comparison dataframe
                comparison_data = {
                    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'],
                    'Train': [
                        train_m.get('accuracy', 0),
                        train_m.get('precision', 0),
                        train_m.get('recall', 0),
                        train_m.get('f1', 0),
                        train_m.get('auc', 0)
                    ],
                    'Test': [
                        test_m.get('accuracy', 0),
                        test_m.get('precision', 0),
                        test_m.get('recall', 0),
                        test_m.get('f1', 0),
                        test_m.get('auc', 0)
                    ]
                }
                
                # Add validation if exists
                if valid_m:
                    comparison_data['Validation'] = [
                        valid_m.get('accuracy', 0),
                        valid_m.get('precision', 0),
                        valid_m.get('recall', 0),
                        valid_m.get('f1', 0),
                        valid_m.get('auc', 0)
                    ]
                    # Reorder columns
                    comparison_df = pd.DataFrame(comparison_data)[['Metric', 'Train', 'Validation', 'Test']]
                else:
                    comparison_df = pd.DataFrame(comparison_data)
                
                # Calculate overfitting indicators (Train - Test gap)
                overfit_threshold = 0.05  # 5% gap considered overfitting
                overfitting_detected = False
                
                for idx, row in comparison_df.iterrows():
                    train_val = row['Train']
                    test_val = row['Test']
                    if train_val - test_val > overfit_threshold:
                        overfitting_detected = True
                        break
                
                # Style the dataframe
                def highlight_overfit(row):
                    train_val = row['Train']
                    test_val = row['Test']
                    gap = train_val - test_val
                    
                    if gap > overfit_threshold:
                        return ['', 'background-color: #4a3535', '', 'background-color: #4a3535']  # Red tint
                    elif gap > 0.02:
                        return ['', 'background-color: #3a3a35', '', 'background-color: #3a3a35']  # Yellow tint
                    else:
                        return ['', 'background-color: #354a35', '', 'background-color: #354a35']  # Green tint
                
                # Display table with formatting
                st.dataframe(
                    comparison_df.style.format({
                        'Train': '{:.4f}',
                        'Validation': '{:.4f}' if valid_m else None,
                        'Test': '{:.4f}'
                    }).apply(lambda x: highlight_overfit(x) if valid_m else ['', '', ''], axis=1),
                    width='stretch',
                    hide_index=True
                )
                
                # Overfitting warning
                if overfitting_detected:
                    st.warning("""
                    ‚ö†Ô∏è **Ph√°t hi·ªán d·∫•u hi·ªáu Overfitting!**
                    - Metrics tr√™n t·∫≠p Train cao h∆°n ƒë√°ng k·ªÉ so v·ªõi Test/Validation (ch√™nh l·ªách > 5%)
                    - **Khuy·∫øn ngh·ªã**: Gi·∫£m ƒë·ªô ph·ª©c t·∫°p model (gi·∫£m max_depth, tƒÉng regularization), ho·∫∑c s·ª≠ d·ª•ng Early Stopping
                    """)
                else:
                    st.success("‚úÖ **Model ·ªïn ƒë·ªãnh**: Kh√¥ng ph√°t hi·ªán d·∫•u hi·ªáu overfitting nghi√™m tr·ªçng.")
            
            st.markdown("---")
            
            # Visualizations
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### üìä Confusion Matrix")
                
                # Mock confusion matrix
                cm = np.array([
                    [850, 150],
                    [100, 900]
                ])
                
                fig = px.imshow(
                    cm,
                    labels=dict(x="Predicted", y="Actual", color="Count"),
                    x=['Negative (0)', 'Positive (1)'],
                    y=['Negative (0)', 'Positive (1)'],
                    text_auto=True,
                    color_continuous_scale='Blues',
                    title="Confusion Matrix"
                )
                
                fig.update_layout(
                    template="plotly_dark",
                    height=400
                )
                
                st.plotly_chart(fig, width='stretch')
                
                # Metrics explanation
                st.markdown("""
                <div style="background-color: #262730; padding: 1rem; border-radius: 8px;">
                    <p style="margin: 0; font-size: 0.9rem;">
                        <strong>True Negative:</strong> {0} | <strong>False Positive:</strong> {1}<br>
                        <strong>False Negative:</strong> {2} | <strong>True Positive:</strong> {3}
                    </p>
                </div>
                """.format(cm[0,0], cm[0,1], cm[1,0], cm[1,1]), unsafe_allow_html=True)
            
            with col2:
                st.markdown("#### üìà ROC Curve")
                
                # Mock ROC curve
                fpr = np.linspace(0, 1, 100)
                tpr = np.sqrt(fpr) * metrics['auc'] + np.random.normal(0, 0.02, 100)
                tpr = np.clip(tpr, 0, 1)
                
                # Get model name for display
                model_name_roc = st.session_state.get('selected_model_name', st.session_state.get('model_type_select', 'Model'))
                
                fig = go.Figure()
                
                # ROC curve
                fig.add_trace(go.Scatter(
                    x=fpr,
                    y=tpr,
                    mode='lines',
                    name=f'{model_name_roc} (AUC = {metrics["auc"]:.3f})',
                    line=dict(color='#667eea', width=3)
                ))
                
                # Diagonal line
                fig.add_trace(go.Scatter(
                    x=[0, 1],
                    y=[0, 1],
                    mode='lines',
                    name='Random (AUC = 0.5)',
                    line=dict(color='red', width=2, dash='dash')
                ))
                
                fig.update_layout(
                    title='ROC Curve',
                    xaxis_title='False Positive Rate',
                    yaxis_title='True Positive Rate',
                    template="plotly_dark",
                    height=400,
                    showlegend=True,
                    legend=dict(x=0.6, y=0.1)
                )
                
                st.plotly_chart(fig, width='stretch')
            
            st.markdown("---")
            
            # Detailed metrics table
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### üìã Classification Report")
                
                report_data = pd.DataFrame({
                    'Class': ['Negative (0)', 'Positive (1)', 'Weighted Avg'],
                    'Precision': [
                        metrics['precision'] - 0.02,
                        metrics['precision'] + 0.02,
                        metrics['precision']
                    ],
                    'Recall': [
                        metrics['recall'] + 0.01,
                        metrics['recall'] - 0.01,
                        metrics['recall']
                    ],
                    'F1-Score': [
                        metrics['f1'] - 0.01,
                        metrics['f1'] + 0.01,
                        metrics['f1']
                    ],
                    'Support': [1000, 1000, 2000]
                })
                
                st.dataframe(
                    report_data.style.format({
                        'Precision': '{:.3f}',
                        'Recall': '{:.3f}',
                        'F1-Score': '{:.3f}'
                    }).background_gradient(subset=['Precision', 'Recall', 'F1-Score'], cmap='Greens'),
                    width='stretch'
                )
            
            with col2:
                st.markdown("#### üìä Precision-Recall Curve")
                
                # Mock PR curve
                recall_vals = np.linspace(0, 1, 100)
                precision_vals = 1 - recall_vals * 0.3 + np.random.normal(0, 0.02, 100)
                precision_vals = np.clip(precision_vals, 0, 1)
                
                # Get model name for display
                model_name_pr = st.session_state.get('selected_model_name', st.session_state.get('model_type_select', 'Model'))
                
                fig = go.Figure()
                
                fig.add_trace(go.Scatter(
                    x=recall_vals,
                    y=precision_vals,
                    mode='lines',
                    fill='tozeroy',
                    name=model_name_pr,
                    line=dict(color='#764ba2', width=3)
                ))
                
                fig.update_layout(
                    title='Precision-Recall Curve',
                    xaxis_title='Recall',
                    yaxis_title='Precision',
                    template="plotly_dark",
                    height=300
                )
                
                st.plotly_chart(fig, width='stretch')
            
            # Download results
            st.markdown("---")
            col1, col2, col3 = st.columns([1, 1, 1])
            
            with col2:
                if st.button("üíæ L∆∞u M√¥ H√¨nh", width='stretch'):
                    st.success("‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh!")
    
    # Tab 3: Model Comparison
    with tab3:
        st.markdown("### üìà L·ªãch S·ª≠ & So S√°nh M√¥ H√¨nh")
        
        if not st.session_state.model_history:
            st.info("üí° Ch∆∞a c√≥ l·ªãch s·ª≠ hu·∫•n luy·ªán. H√£y hu·∫•n luy·ªán √≠t nh·∫•t m·ªôt m√¥ h√¨nh ·ªü Tab 1.")
        else:
            st.markdown(f"ƒê√£ l∆∞u {len(st.session_state.model_history)} k·∫øt qu·∫£ hu·∫•n luy·ªán.")
            
            # Convert history to DataFrame
            history_df = pd.DataFrame(st.session_state.model_history)
            
            # Display table with selection
            st.markdown("#### üìä B·∫£ng So S√°nh Chi Ti·∫øt")
            
            # Get currently selected model index
            selected_model_idx = st.session_state.get('selected_model_idx', None)
            
            # Create selection options
            model_options = [f"{i}: {row['Model']} ({row['Timestamp']}) - AUC: {row['AUC']:.3f}" 
                           for i, row in history_df.iterrows()]
            
            # Add "Ch∆∞a ch·ªçn" option at the beginning
            model_options_with_none = ["-- Ch·ªçn m√¥ h√¨nh --"] + model_options
            
            # Determine default index
            if selected_model_idx is not None and selected_model_idx < len(model_options):
                default_idx = selected_model_idx + 1  # +1 because of "Ch∆∞a ch·ªçn" option
            else:
                default_idx = 0
            
            # Selection dropdown
            selected_option = st.selectbox(
                "üéØ Ch·ªçn m√¥ h√¨nh ƒë·ªÉ s·ª≠ d·ª•ng:",
                model_options_with_none,
                index=default_idx,
                key="model_selector"
            )
            
            # Handle selection
            if selected_option != "-- Ch·ªçn m√¥ h√¨nh --":
                # Extract index from selection
                new_idx = int(selected_option.split(":")[0])
                
                if new_idx != selected_model_idx:
                    row = history_df.iloc[new_idx]
                    st.session_state.selected_model_idx = new_idx
                    st.session_state.selected_model_name = row['Model']
                    st.session_state.selected_model_timestamp = row['Timestamp']
                    st.session_state.model_metrics = {
                        'accuracy': row['Accuracy'],
                        'precision': row['Precision'],
                        'recall': row['Recall'],
                        'f1': row['F1-Score'],
                        'auc': row['AUC']
                    }
                    st.session_state.explainer = None
                    st.session_state.shap_values = None
                    st.rerun()
            
            # Display the dataframe
            st.dataframe(
                history_df.style.format({
                    'Accuracy': '{:.3f}',
                    'Precision': '{:.3f}',
                    'Recall': '{:.3f}',
                    'F1-Score': '{:.3f}',
                    'AUC': '{:.3f}'
                }).background_gradient(subset=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'], cmap='RdYlGn'),
                width='stretch'
            )
            
            st.markdown("---")
            
            # Show selected model info
            if selected_model_idx is not None and selected_model_idx < len(st.session_state.model_history):
                selected_info = st.session_state.model_history[selected_model_idx]
                st.success(f"‚úÖ **M√¥ h√¨nh ƒë∆∞·ª£c ch·ªçn**: {selected_info['Model']} (Timestamp: {selected_info['Timestamp']}) - AUC: {selected_info['AUC']:.3f}")
                st.info("üí° B·∫°n c√≥ th·ªÉ chuy·ªÉn sang trang **Model Explanation** ƒë·ªÉ xem gi·∫£i th√≠ch m√¥ h√¨nh.")
            else:
                st.info("üí° Ch·ªçn m√¥ h√¨nh t·ª´ dropdown ·ªü tr√™n ƒë·ªÉ s·ª≠ d·ª•ng cho Model Explanation.")
            
            # Comparison charts
            st.markdown("#### üìâ Bi·ªÉu ƒê·ªì So S√°nh")
            col1, col2 = st.columns(2)
            
            with col1:
                # Bar chart comparison
                fig = go.Figure()
                
                metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
                
                # Group by Model to get average or max if multiple runs of same model? 
                # For now, just plot all runs, maybe add ID or Timestamp to x-axis
                history_df['Run ID'] = history_df['Model'] + " (" + history_df['Timestamp'] + ")"
                
                for metric in metrics_to_plot:
                    fig.add_trace(go.Bar(
                        name=metric,
                        x=history_df['Run ID'],
                        y=history_df[metric],
                        text=history_df[metric].round(3),
                        textposition='outside'
                    ))
                
                fig.update_layout(
                    title='So S√°nh Metrics Gi·ªØa C√°c L·∫ßn Ch·∫°y',
                    xaxis_title='Model Run',
                    yaxis_title='Score',
                    template="plotly_dark",
                    height=400,
                    barmode='group'
                )
                
                st.plotly_chart(fig, width='stretch')
            
            with col2:
                # Line chart for AUC trend
                fig = px.line(
                    history_df, 
                    x='Timestamp', 
                    y='AUC', 
                    color='Model',
                    markers=True,
                    title='Xu H∆∞·ªõng AUC Theo Th·ªùi Gian'
                )
                fig.update_layout(template="plotly_dark", height=400)
                st.plotly_chart(fig, width='stretch')
            
            # Best model recommendation
            best_run_idx = history_df['AUC'].idxmax()
            best_model = history_df.loc[best_run_idx, 'Model']
            best_auc = history_df.loc[best_run_idx, 'AUC']
            best_time = history_df.loc[best_run_idx, 'Timestamp']
            
            st.success(f"üèÜ **M√¥ h√¨nh t·ªët nh·∫•t hi·ªán t·∫°i**: {best_model} (ch·∫°y l√∫c {best_time}) v·ªõi AUC = {best_auc:.3f}")
            
            # Clear history button
            if st.button("üóëÔ∏è X√≥a L·ªãch S·ª≠", type="secondary"):
                st.session_state.model_history = []
                st.rerun()

