"""
EDA Analyzer - Thu th·∫≠p v√† ph√¢n t√≠ch d·ªØ li·ªáu EDA b·∫±ng LLM
"""
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional
import json


class EDADataCollector:
    """Thu th·∫≠p to√†n b·ªô th√¥ng tin t·ª´ qu√° tr√¨nh EDA"""
    
    def __init__(self, data: pd.DataFrame):
        self.data = data
        self.summary = {}
        
    def collect_basic_info(self) -> Dict[str, Any]:
        """Thu th·∫≠p th√¥ng tin c∆° b·∫£n v·ªÅ dataset"""
        return {
            "n_rows": len(self.data),
            "n_columns": len(self.data.columns),
            "columns": self.data.columns.tolist(),
            "dtypes": self.data.dtypes.astype(str).to_dict(),
            "memory_usage_mb": self.data.memory_usage(deep=True).sum() / 1024**2
        }
    
    def collect_missing_data(self) -> Dict[str, Any]:
        """Ph√¢n t√≠ch d·ªØ li·ªáu thi·∫øu"""
        missing = self.data.isnull().sum()
        missing_pct = (missing / len(self.data) * 100).round(2)
        
        return {
            "total_missing": int(missing.sum()),
            "missing_by_column": {
                col: {
                    "count": int(missing[col]),
                    "percentage": float(missing_pct[col])
                }
                for col in self.data.columns if missing[col] > 0
            },
            "columns_with_missing": missing[missing > 0].index.tolist(),
            "complete_rows": int((~self.data.isnull().any(axis=1)).sum())
        }
    
    def collect_numeric_stats(self) -> Dict[str, Any]:
        """Th·ªëng k√™ chi ti·∫øt cho bi·∫øn s·ªë"""
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        stats = {}
        
        for col in numeric_cols:
            col_data = self.data[col].dropna()
            
            if len(col_data) == 0:
                continue
                
            # Calculate statistics
            q1 = col_data.quantile(0.25)
            q3 = col_data.quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]
            
            # Skewness and kurtosis
            try:
                from scipy import stats as scipy_stats
                skewness = float(scipy_stats.skew(col_data))
                kurtosis = float(scipy_stats.kurtosis(col_data))
            except:
                skewness = None
                kurtosis = None
            
            stats[col] = {
                "count": int(col_data.count()),
                "mean": float(col_data.mean()),
                "median": float(col_data.median()),
                "std": float(col_data.std()),
                "min": float(col_data.min()),
                "max": float(col_data.max()),
                "q1": float(q1),
                "q3": float(q3),
                "iqr": float(iqr),
                "outliers_count": len(outliers),
                "outliers_pct": round(len(outliers) / len(col_data) * 100, 2),
                "skewness": skewness,
                "kurtosis": kurtosis,
                "cv": round(col_data.std() / col_data.mean() * 100, 2) if col_data.mean() != 0 else None,
                "zeros_count": int((col_data == 0).sum()),
                "zeros_pct": round((col_data == 0).sum() / len(col_data) * 100, 2)
            }
        
        return stats
    
    def collect_categorical_stats(self) -> Dict[str, Any]:
        """Th·ªëng k√™ chi ti·∫øt cho bi·∫øn ph√¢n lo·∫°i"""
        cat_cols = self.data.select_dtypes(include=['object', 'category']).columns
        stats = {}
        
        for col in cat_cols:
            col_data = self.data[col].dropna()
            
            if len(col_data) == 0:
                continue
            
            value_counts = col_data.value_counts()
            
            stats[col] = {
                "unique_count": int(col_data.nunique()),
                "most_common": value_counts.index[0] if len(value_counts) > 0 else None,
                "most_common_count": int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
                "most_common_pct": round(value_counts.iloc[0] / len(col_data) * 100, 2) if len(value_counts) > 0 else 0,
                "top_5_values": value_counts.head(5).to_dict(),
                "is_high_cardinality": col_data.nunique() > len(col_data) * 0.5,
                "entropy": float(-sum((value_counts / len(col_data)) * np.log2(value_counts / len(col_data))))
            }
        
        return stats
    
    def collect_correlations(self, threshold: float = 0.5) -> Dict[str, Any]:
        """Ph√¢n t√≠ch t∆∞∆°ng quan gi·ªØa c√°c bi·∫øn"""
        numeric_data = self.data.select_dtypes(include=[np.number])
        
        if len(numeric_data.columns) < 2:
            return {"message": "Kh√¥ng ƒë·ªß bi·∫øn s·ªë ƒë·ªÉ t√≠nh t∆∞∆°ng quan"}
        
        corr_matrix = numeric_data.corr()
        
        # Find high correlations
        high_corr = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) >= threshold:
                    high_corr.append({
                        "var1": corr_matrix.columns[i],
                        "var2": corr_matrix.columns[j],
                        "correlation": round(float(corr_val), 3),
                        "type": "positive" if corr_val > 0 else "negative",
                        "strength": "strong" if abs(corr_val) >= 0.7 else "moderate"
                    })
        
        # Sort by absolute correlation
        high_corr.sort(key=lambda x: abs(x['correlation']), reverse=True)
        
        return {
            "correlation_matrix_shape": corr_matrix.shape,
            "high_correlations": high_corr,
            "avg_correlation": round(float(corr_matrix.abs().mean().mean()), 3),
            "max_correlation": round(float(corr_matrix.abs().max().max()), 3) if len(corr_matrix) > 1 else 0
        }
    
    def collect_data_quality_issues(self) -> Dict[str, List[str]]:
        """Ph√°t hi·ªán c√°c v·∫•n ƒë·ªÅ v·ªÅ ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu"""
        issues = {
            "high_missing": [],
            "high_cardinality": [],
            "potential_id_columns": [],
            "constant_columns": [],
            "high_outliers": [],
            "highly_skewed": [],
            "duplicate_rows": []
        }
        
        # High missing values (>30%)
        missing_pct = (self.data.isnull().sum() / len(self.data) * 100)
        issues["high_missing"] = missing_pct[missing_pct > 30].index.tolist()
        
        # High cardinality categorical
        for col in self.data.select_dtypes(include=['object', 'category']).columns:
            if self.data[col].nunique() > len(self.data) * 0.5:
                issues["high_cardinality"].append(col)
        
        # Potential ID columns (all unique numeric)
        for col in self.data.select_dtypes(include=[np.number]).columns:
            if self.data[col].nunique() == len(self.data):
                issues["potential_id_columns"].append(col)
        
        # Constant columns (only 1 unique value)
        for col in self.data.columns:
            if self.data[col].nunique() == 1:
                issues["constant_columns"].append(col)
        
        # High outliers (>10% outliers)
        for col in self.data.select_dtypes(include=[np.number]).columns:
            col_data = self.data[col].dropna()
            if len(col_data) > 0:
                q1 = col_data.quantile(0.25)
                q3 = col_data.quantile(0.75)
                iqr = q3 - q1
                outliers = col_data[(col_data < q1 - 1.5 * iqr) | (col_data > q3 + 1.5 * iqr)]
                if len(outliers) / len(col_data) > 0.1:
                    issues["high_outliers"].append(col)
        
        # Highly skewed (|skewness| > 2)
        try:
            from scipy import stats as scipy_stats
            for col in self.data.select_dtypes(include=[np.number]).columns:
                col_data = self.data[col].dropna()
                if len(col_data) > 0:
                    skew = scipy_stats.skew(col_data)
                    if abs(skew) > 2:
                        issues["highly_skewed"].append(col)
        except:
            pass
        
        # Duplicate rows
        duplicates = self.data.duplicated().sum()
        if duplicates > 0:
            issues["duplicate_rows"] = [f"{duplicates} duplicate rows found ({round(duplicates/len(self.data)*100, 2)}%)"]
        
        return issues
    
    def generate_full_summary(self) -> Dict[str, Any]:
        """T·∫°o b√°o c√°o t·ªïng h·ª£p to√†n b·ªô EDA"""
        return {
            "basic_info": self.collect_basic_info(),
            "missing_data": self.collect_missing_data(),
            "numeric_stats": self.collect_numeric_stats(),
            "categorical_stats": self.collect_categorical_stats(),
            "correlations": self.collect_correlations(),
            "data_quality_issues": self.collect_data_quality_issues()
        }
    
    def to_text_summary(self) -> str:
        """Chuy·ªÉn ƒë·ªïi summary th√†nh vƒÉn b·∫£n d·ªÖ ƒë·ªçc cho LLM"""
        summary = self.generate_full_summary()
        
        text_parts = []
        
        # Basic Info
        text_parts.append("=" * 80)
        text_parts.append("TH√îNG TIN C∆† B·∫¢N V·ªÄ DATASET")
        text_parts.append("=" * 80)
        basic = summary['basic_info']
        text_parts.append(f"S·ªë d√≤ng: {basic['n_rows']:,}")
        text_parts.append(f"S·ªë c·ªôt: {basic['n_columns']}")
        text_parts.append(f"Dung l∆∞·ª£ng: {basic['memory_usage_mb']:.2f} MB")
        text_parts.append(f"\nC√°c c·ªôt: {', '.join(basic['columns'])}")
        
        # Missing Data
        text_parts.append("\n" + "=" * 80)
        text_parts.append("PH√ÇN T√çCH D·ªÆ LI·ªÜU THI·∫æU")
        text_parts.append("=" * 80)
        missing = summary['missing_data']
        text_parts.append(f"T·ªïng gi√° tr·ªã thi·∫øu: {missing['total_missing']:,}")
        text_parts.append(f"S·ªë d√≤ng ho√†n ch·ªânh: {missing['complete_rows']:,}")
        if missing['missing_by_column']:
            text_parts.append("\nC√°c c·ªôt c√≥ d·ªØ li·ªáu thi·∫øu:")
            for col, info in missing['missing_by_column'].items():
                text_parts.append(f"  - {col}: {info['count']} ({info['percentage']:.2f}%)")
        
        # Numeric Stats
        text_parts.append("\n" + "=" * 80)
        text_parts.append("TH·ªêNG K√ä BI·∫æN S·ªê")
        text_parts.append("=" * 80)
        for col, stats in summary['numeric_stats'].items():
            text_parts.append(f"\n{col}:")
            text_parts.append(f"  Mean: {stats['mean']:.2f}, Median: {stats['median']:.2f}, Std: {stats['std']:.2f}")
            text_parts.append(f"  Range: [{stats['min']:.2f}, {stats['max']:.2f}]")
            text_parts.append(f"  Outliers: {stats['outliers_count']} ({stats['outliers_pct']:.2f}%)")
            if stats['skewness'] is not None:
                text_parts.append(f"  Skewness: {stats['skewness']:.2f}, Kurtosis: {stats['kurtosis']:.2f}")
            if stats['cv'] is not None:
                text_parts.append(f"  Coefficient of Variation: {stats['cv']:.2f}%")
        
        # Categorical Stats
        text_parts.append("\n" + "=" * 80)
        text_parts.append("TH·ªêNG K√ä BI·∫æN PH√ÇN LO·∫†I")
        text_parts.append("=" * 80)
        for col, stats in summary['categorical_stats'].items():
            text_parts.append(f"\n{col}:")
            text_parts.append(f"  S·ªë gi√° tr·ªã kh√°c nhau: {stats['unique_count']}")
            text_parts.append(f"  Gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t: {stats['most_common']} ({stats['most_common_pct']:.2f}%)")
            text_parts.append(f"  High cardinality: {'C√≥' if stats['is_high_cardinality'] else 'Kh√¥ng'}")
        
        # Correlations
        text_parts.append("\n" + "=" * 80)
        text_parts.append("PH√ÇN T√çCH T∆Ø∆†NG QUAN")
        text_parts.append("=" * 80)
        corr = summary['correlations']
        if 'high_correlations' in corr and corr['high_correlations']:
            text_parts.append(f"S·ªë c·∫∑p bi·∫øn c√≥ t∆∞∆°ng quan cao (‚â•0.5): {len(corr['high_correlations'])}")
            text_parts.append("\nTop correlations:")
            for item in corr['high_correlations'][:10]:
                text_parts.append(f"  - {item['var1']} ‚Üî {item['var2']}: {item['correlation']:.3f} ({item['strength']}, {item['type']})")
        
        # Data Quality Issues
        text_parts.append("\n" + "=" * 80)
        text_parts.append("V·∫§N ƒê·ªÄ CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU")
        text_parts.append("=" * 80)
        issues = summary['data_quality_issues']
        for issue_type, issue_list in issues.items():
            if issue_list:
                text_parts.append(f"\n{issue_type.replace('_', ' ').title()}:")
                for item in issue_list:
                    text_parts.append(f"  - {item}")
        
        return "\n".join(text_parts)


class LLMEDAAnalyzer:
    """S·ª≠ d·ª•ng LLM ƒë·ªÉ ph√¢n t√≠ch k·∫øt qu·∫£ EDA"""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gemini-2.5-flash", provider: str = "google"):
        """
        Initialize LLM analyzer
        
        Args:
            api_key: API key for LLM service (OpenAI, Anthropic, Google)
            model: Model name to use
            provider: LLM provider ('openai', 'anthropic', 'google')
        """
        self.api_key = api_key
        self.model = model
        self.provider = provider
        self.client = None
        
    def _init_client(self):
        """Initialize LLM client based on provider"""
        if self.api_key is None:
            raise ValueError("API key is required. Set it in environment or pass to constructor.")
        
        if self.provider == "google":
            try:
                import google.generativeai as genai
                genai.configure(api_key=self.api_key)
                self.client = genai.GenerativeModel(self.model)
            except ImportError:
                raise ImportError("Google Generative AI library not installed. Run: pip install google-generativeai")
        
        elif self.provider == "openai":
            try:
                from openai import OpenAI
                self.client = OpenAI(api_key=self.api_key)
            except ImportError:
                raise ImportError("OpenAI library not installed. Run: pip install openai")
        
        elif self.provider == "anthropic":
            try:
                import anthropic
                self.client = anthropic.Anthropic(api_key=self.api_key)
            except ImportError:
                raise ImportError("Anthropic library not installed. Run: pip install anthropic")
    
    def create_analysis_prompt(self, eda_summary: str) -> str:
        """T·∫°o prompt cho LLM ƒë·ªÉ ph√¢n t√≠ch EDA"""
        prompt = f"""B·∫°n l√† m·ªôt Data Scientist chuy√™n nghi·ªáp v·ªõi nhi·ªÅu nƒÉm kinh nghi·ªám trong ph√¢n t√≠ch d·ªØ li·ªáu v√† x√¢y d·ª±ng m√¥ h√¨nh Credit Scoring.

D·ª±a tr√™n k·∫øt qu·∫£ EDA (Exploratory Data Analysis) d∆∞·ªõi ƒë√¢y, h√£y cung c·∫•p m·ªôt ph√¢n t√≠ch chi ti·∫øt v√† ƒë·ªÅ xu·∫•t c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω c·∫ßn thi·∫øt.

{eda_summary}

H√£y ph√¢n t√≠ch v√† tr·∫£ l·ªùi theo c·∫•u tr√∫c sau:

## 1. ƒê√ÅNH GI√Å T·ªîNG QUAN
- Ch·∫•t l∆∞·ª£ng t·ªïng th·ªÉ c·ªßa dataset
- C√°c ƒëi·ªÉm m·∫°nh v√† ƒëi·ªÉm y·∫øu
- M·ª©c ƒë·ªô s·∫µn s√†ng cho modeling

## 2. PH√ÇN T√çCH CHI TI·∫æT

### 2.1 D·ªØ Li·ªáu Thi·∫øu (Missing Data)
- ƒê√°nh gi√° m·ª©c ƒë·ªô nghi√™m tr·ªçng
- Nguy√™n nh√¢n c√≥ th·ªÉ
- ƒê·ªÅ xu·∫•t ph∆∞∆°ng ph√°p x·ª≠ l√Ω (imputation, deletion, etc.)

### 2.2 Bi·∫øn S·ªë (Numeric Variables)
- Ph√¢n ph·ªëi c·ªßa c√°c bi·∫øn (normal, skewed, etc.)
- Outliers v√† c√°ch x·ª≠ l√Ω
- C√°c bi·∫øn c·∫ßn transformation (log, sqrt, standardization, etc.)

### 2.3 Bi·∫øn Ph√¢n Lo·∫°i (Categorical Variables)
- V·∫•n ƒë·ªÅ v·ªÅ cardinality
- Encoding strategy (one-hot, label, target encoding)
- X·ª≠ l√Ω rare categories

### 2.4 T∆∞∆°ng Quan (Correlations)
- Multicollinearity issues
- Feature selection recommendations
- Potential feature engineering opportunities

## 3. V·∫§N ƒê·ªÄ C·∫¶N ∆ØU TI√äN X·ª¨ L√ù
Li·ªát k√™ c√°c v·∫•n ƒë·ªÅ theo th·ª© t·ª± ∆∞u ti√™n:
1. [V·∫•n ƒë·ªÅ 1]
2. [V·∫•n ƒë·ªÅ 2]
...

## 4. ROADMAP TI·ªÄN X·ª¨ L√ù
ƒê·ªÅ xu·∫•t c√°c b∆∞·ªõc c·ª• th·ªÉ c·∫ßn th·ª±c hi·ªán:
- B∆∞·ªõc 1: ...
- B∆∞·ªõc 2: ...
...

## 5. K·∫æT LU·∫¨N
- T√≥m t·∫Øt ƒë√°nh gi√°
- D·ª± ƒëo√°n kh·∫£ nƒÉng x√¢y d·ª±ng m√¥ h√¨nh t·ªët
- C√°c l∆∞u √Ω ƒë·∫∑c bi·ªát

H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, chuy√™n nghi·ªáp nh∆∞ng d·ªÖ hi·ªÉu."""

        return prompt
    
    def analyze(self, data: pd.DataFrame, use_cached: bool = True) -> str:
        """
        Ph√¢n t√≠ch d·ªØ li·ªáu EDA b·∫±ng LLM
        
        Args:
            data: DataFrame c·∫ßn ph√¢n t√≠ch
            use_cached: S·ª≠ d·ª•ng k·∫øt qu·∫£ cached n·∫øu c√≥
            
        Returns:
            Ph√¢n t√≠ch chi ti·∫øt t·ª´ LLM (markdown format)
        """
        # Collect EDA data
        collector = EDADataCollector(data)
        eda_summary = collector.to_text_summary()
        
        # If no API key, return template analysis
        if self.api_key is None:
            return self._generate_template_analysis(collector.generate_full_summary())
        
        # Initialize client
        if self.client is None:
            self._init_client()
        
        # Create prompt
        prompt = self.create_analysis_prompt(eda_summary)
        
        # Call LLM based on provider
        try:
            if self.provider == "google":
                # Google Gemini
                response = self.client.generate_content(prompt)
                return response.text
            
            elif self.provider == "openai":
                # OpenAI GPT
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "B·∫°n l√† m·ªôt Data Scientist chuy√™n nghi·ªáp."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=8000
                )
                return response.choices[0].message.content
            
            elif self.provider == "anthropic":
                # Anthropic Claude
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=8000,
                    temperature=0.7,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                return response.content[0].text
            
            else:
                return f"‚ùå **Provider kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£**: {self.provider}"
        
        except Exception as e:
            return f"‚ùå **L·ªói khi g·ªçi LLM**: {str(e)}\n\nVui l√≤ng ki·ªÉm tra API key v√† th·ª≠ l·∫°i."
    
    def _generate_template_analysis(self, summary: Dict[str, Any]) -> str:
        """T·∫°o ph√¢n t√≠ch m·∫´u khi kh√¥ng c√≥ API key"""
        basic = summary['basic_info']
        missing = summary['missing_data']
        issues = summary['data_quality_issues']
        
        template = f"""## üîç PH√ÇN T√çCH T·ª∞ ƒê·ªòNG (TEMPLATE MODE)

> ‚ö†Ô∏è **L∆∞u √Ω**: ƒê√¢y l√† ph√¢n t√≠ch t·ª± ƒë·ªông c∆° b·∫£n. ƒê·ªÉ c√≥ ph√¢n t√≠ch chi ti·∫øt t·ª´ AI, vui l√≤ng c·∫•u h√¨nh API key.

### 1. ƒê√ÅNH GI√Å T·ªîNG QUAN

Dataset c√≥ **{basic['n_rows']:,} d√≤ng** v√† **{basic['n_columns']} c·ªôt**.

**Ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu**: {'‚ö†Ô∏è C·∫ßn c·∫£i thi·ªán' if missing['total_missing'] > 0 else '‚úÖ T·ªët'}

### 2. V·∫§N ƒê·ªÄ PH√ÅT HI·ªÜN

"""
        
        # Missing data issues
        if missing['total_missing'] > 0:
            template += f"\n#### üìâ D·ªØ Li·ªáu Thi·∫øu\n"
            template += f"- T·ªïng: **{missing['total_missing']:,}** gi√° tr·ªã thi·∫øu\n"
            template += f"- S·ªë c·ªôt b·ªã ·∫£nh h∆∞·ªüng: **{len(missing['columns_with_missing'])}**\n"
        
        # Data quality issues
        critical_issues = []
        if issues['high_missing']:
            critical_issues.append(f"**{len(issues['high_missing'])} c·ªôt** c√≥ >30% d·ªØ li·ªáu thi·∫øu")
        if issues['constant_columns']:
            critical_issues.append(f"**{len(issues['constant_columns'])} c·ªôt** c√≥ gi√° tr·ªã kh√¥ng ƒë·ªïi")
        if issues['high_outliers']:
            critical_issues.append(f"**{len(issues['high_outliers'])} c·ªôt** c√≥ nhi·ªÅu outliers (>10%)")
        if issues['highly_skewed']:
            critical_issues.append(f"**{len(issues['highly_skewed'])} c·ªôt** c√≥ ph√¢n ph·ªëi l·ªách m·∫°nh")
        
        if critical_issues:
            template += "\n#### ‚ö†Ô∏è V·∫•n ƒê·ªÅ C·∫ßn X·ª≠ L√Ω\n"
            for issue in critical_issues:
                template += f"- {issue}\n"
        
        # Recommendations
        template += "\n### 3. ƒê·ªÄ XU·∫§T TI·ªÄN X·ª¨ L√ù\n\n"
        
        if missing['total_missing'] > 0:
            template += "**X·ª≠ l√Ω d·ªØ li·ªáu thi·∫øu:**\n"
            template += "- Xem x√©t imputation (mean/median cho s·ªë, mode cho categorical)\n"
            template += "- Ho·∫∑c lo·∫°i b·ªè c√°c d√≤ng/c·ªôt c√≥ qu√° nhi·ªÅu missing\n\n"
        
        if issues['high_outliers']:
            template += "**X·ª≠ l√Ω outliers:**\n"
            template += "- C√¢n nh·∫Øc winsorization ho·∫∑c transformation (log, sqrt)\n"
            template += "- Ki·ªÉm tra xem c√≥ ph·∫£i outliers h·ª£p l·ªá kh√¥ng\n\n"
        
        if issues['highly_skewed']:
            template += "**X·ª≠ l√Ω ph√¢n ph·ªëi l·ªách:**\n"
            template += "- √Åp d·ª•ng log/sqrt transformation\n"
            template += "- Xem x√©t standardization sau transformation\n\n"
        
        template += "\n### 4. K·∫æT LU·∫¨N\n\n"
        template += "Dataset c·∫ßn **ti·ªÅn x·ª≠ l√Ω** tr∆∞·ªõc khi training model. "
        template += "H√£y th·ª±c hi·ªán c√°c b∆∞·ªõc ƒë·ªÅ xu·∫•t ·ªü ph·∫ßn Feature Engineering.\n\n"
        template += "---\n"
        template += "*üí° ƒê·ªÉ c√≥ ph√¢n t√≠ch chi ti·∫øt h∆°n t·ª´ AI, h√£y c·∫•u h√¨nh OpenAI API key trong file `.env`*"
        
        return template


# Utility functions
def analyze_eda_with_llm(data: pd.DataFrame, api_key: Optional[str] = None, provider: str = "google") -> str:
    """
    Quick function ƒë·ªÉ ph√¢n t√≠ch EDA
    
    Args:
        data: DataFrame to analyze
        api_key: API key (if None, will use from config)
        provider: LLM provider ('google', 'openai', 'anthropic')
    
    Usage:
        # Google Gemini
        analysis = analyze_eda_with_llm(df, api_key="...", provider="google")
        
        # OpenAI GPT
        analysis = analyze_eda_with_llm(df, api_key="sk-...", provider="openai")
    """
    from .config import LLMConfig
    
    # Get API key and model from config if not provided
    if api_key is None:
        api_key = LLMConfig.get_api_key(provider)
    
    model = LLMConfig.get_model(provider)
    
    analyzer = LLMEDAAnalyzer(api_key=api_key, model=model, provider=provider)
    return analyzer.analyze(data)


def get_eda_summary(data: pd.DataFrame, format: str = "text") -> str:
    """
    L·∫•y summary c·ªßa EDA
    
    Args:
        data: DataFrame
        format: "text" or "json"
    """
    collector = EDADataCollector(data)
    
    if format == "json":
        return json.dumps(collector.generate_full_summary(), indent=2, ensure_ascii=False)
    else:
        return collector.to_text_summary()
